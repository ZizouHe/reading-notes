
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Graph Convolution and Graph Attention Â· Reading List & Notes</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-image-captions/image-captions.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchors/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="phack.html" />
    
    
    <link rel="prev" href="retail.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"ZizouHe","repo":"reading-notes","type":"star","size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../contents/already.html">
            
                <a href="../contents/already.html">
            
                    
                    Finished List
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../contents/bayes.html">
            
                <a href="../contents/bayes.html">
            
                    
                    Bayesian Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../contents/DL.html">
            
                <a href="../contents/DL.html">
            
                    
                    Deep Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../contents/fin.html">
            
                <a href="../contents/fin.html">
            
                    
                    Finance/Quant Finance
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../contents/CS.html">
            
                <a href="../contents/CS.html">
            
                    
                    General CS
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="../contents/DS.html">
            
                <a href="../contents/DS.html">
            
                    
                    General Data Science
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../contents/ML.html">
            
                <a href="../contents/ML.html">
            
                    
                    Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../contents/opt.html">
            
                <a href="../contents/opt.html">
            
                    
                    Numerical and Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="../contents/stat.html">
            
                <a href="../contents/stat.html">
            
                    
                    Traditional Statistics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="../contents/book.html">
            
                <a href="../contents/book.html">
            
                    
                    Books
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10" data-path="../contents/broker.html">
            
                <a href="../contents/broker.html">
            
                    
                    Broker Reports
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11" data-path="../contents/qfn.html">
            
                <a href="../contents/qfn.html">
            
                    
                    Quant Finance Notes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12" data-path="../contents/mln.html">
            
                <a href="../contents/mln.html">
            
                    
                    Machine Learning Notes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13" data-path="temp.html">
            
                <a href="temp.html">
            
                    
                    Temp files
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../contents/reading.html">
            
                <a href="../contents/reading.html">
            
                    
                    Reading List
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../contents/bayesing.html">
            
                <a href="../contents/bayesing.html">
            
                    
                    Bayesian Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../contents/DLing.html">
            
                <a href="../contents/DLing.html">
            
                    
                    Deep Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="../contents/MLing.html">
            
                <a href="../contents/MLing.html">
            
                    
                    Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="../contents/opting.html">
            
                <a href="../contents/opting.html">
            
                    
                    Numerical and Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="../contents/stating.html">
            
                <a href="../contents/stating.html">
            
                    
                    Traditional Statistics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="../contents/booking.html">
            
                <a href="../contents/booking.html">
            
                    
                    Books
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../contents/notes.html">
            
                <a href="../contents/notes.html">
            
                    
                    Notes
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="NeuralODE.html">
            
                <a href="NeuralODE.html">
            
                    
                    Neural ODE
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="RDN.html">
            
                <a href="RDN.html">
            
                    
                    Relational Dependency Networks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="LDA.html">
            
                <a href="LDA.html">
            
                    
                    LDA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="KSD_test.html">
            
                <a href="KSD_test.html">
            
                    
                    KSD for Goodness-of-fit Tests
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="LTK_test.html">
            
                <a href="LTK_test.html">
            
                    
                    LTK Goodness-of-Fit Test
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="GaussMixCompress.html">
            
                <a href="GaussMixCompress.html">
            
                    
                    Sample Complexity Bounds for Gaussian Mixtures
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="SGMCMC.html">
            
                <a href="SGMCMC.html">
            
                    
                    A Complete Recipe for SGMCMC
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.8" data-path="Sharpe_Minima_Works.html">
            
                <a href="Sharpe_Minima_Works.html">
            
                    
                    Sharpe Minima Can Generalize
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.9" data-path="Sharpe_Minima_Exp.html">
            
                <a href="Sharpe_Minima_Exp.html">
            
                    
                    Large Batch Training and Sharpe Minima
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.10" data-path="RMLHMC.html">
            
                <a href="RMLHMC.html">
            
                    
                    Riemann Manifold Langevin and HMC
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.11" data-path="pathSGD.html">
            
                <a href="pathSGD.html">
            
                    
                    Path-SGD: Path-normalized Optimization in Deep Neural Networks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.12" data-path="Empirical_Loss.html">
            
                <a href="Empirical_Loss.html">
            
                    
                    Empirical Analysis of DNN Loss Surfaces
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.13" data-path="Good_Semi_Bad_GAN.html">
            
                <a href="Good_Semi_Bad_GAN.html">
            
                    
                    Good Semi-supervised Learning that Requires a Bad GAN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.14" data-path="Faster_Frank_Wolfe.html">
            
                <a href="Faster_Frank_Wolfe.html">
            
                    
                    Faster Rates for Frank-Wolfe over Strongly-Convex Sets
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.15" data-path="Fin_Fan_Book.html">
            
                <a href="Fin_Fan_Book.html">
            
                    
                    The Element of Financial Econometrics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.16" data-path="Double_ML.html">
            
                <a href="Double_ML.html">
            
                    
                    Double machine learning for treatment effect
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.17" data-path="RL_Book1.html">
            
                <a href="RL_Book1.html">
            
                    
                    Part 1: Reinforcement Learning: An Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.18" data-path="RL_Book2.html">
            
                <a href="RL_Book2.html">
            
                    
                    Part 2: Reinforcement Learning: An Introduction (2)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.19" data-path="TRPO.html">
            
                <a href="TRPO.html">
            
                    
                    Trust Region Policy Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.20" data-path="Prior_Exp_Replay.html">
            
                <a href="Prior_Exp_Replay.html">
            
                    
                    Prioritized Experience Replay
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.21" data-path="stein-galton.html">
            
                <a href="stein-galton.html">
            
                    
                    A Galtonian Perspective on Shrinkage Estimators
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.22" data-path="lasso.html">
            
                <a href="lasso.html">
            
                    
                    Regression Shrinkage and Selection via the Lasso
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.23" data-path="nonpa_inst.html">
            
                <a href="nonpa_inst.html">
            
                    
                    Nonparametric Instrumental Regression
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.24" data-path="defi_RFS.html">
            
                <a href="defi_RFS.html">
            
                    
                    Decentralized mining in centralized pools
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.25" data-path="ES_2014_majority.html">
            
                <a href="ES_2014_majority.html">
            
                    
                    Majority is not enough: Bitcoin mining is vulnerable
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.26" data-path="lazy-prices.html">
            
                <a href="lazy-prices.html">
            
                    
                    Lazy Prices
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.27" data-path="wager_athey_pnas_2021.html">
            
                <a href="wager_athey_pnas_2021.html">
            
                    
                    Confidence Intervals for Policy Evaluation in Adaptive Experiments
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.28" data-path="kelly_zhang_2021_nips.html">
            
                <a href="kelly_zhang_2021_nips.html">
            
                    
                    Statistical Inference with M-Estimators on Adaptively Collected Data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.29" data-path="realized_var.html">
            
                <a href="realized_var.html">
            
                    
                    Realized Variance and Market Microstructure Noise
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.30" data-path="PoS.html">
            
                <a href="PoS.html">
            
                    
                    Blockchain without waste: Proof-of-stake
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.31" data-path="green_2019_jfe.html">
            
                <a href="green_2019_jfe.html">
            
                    
                    Crowdsourced employer reviews and stock returns
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.32" data-path="SoK.html">
            
                <a href="SoK.html">
            
                    
                    A Survey of Attacks on Ethereum Smart Contracts
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.33" data-path="Lee_2019_jfe.html">
            
                <a href="Lee_2019_jfe.html">
            
                    
                    Technological Links and Predictable Returns
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.34" data-path="Da_2021_jfe.html">
            
                <a href="Da_2021_jfe.html">
            
                    
                    Extrapolative beliefs in the cross-section: What can we learn from the crowds?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.35" data-path="Parsons_2020_rfs.html">
            
                <a href="Parsons_2020_rfs.html">
            
                    
                    Geographic Lead-Lag Effects
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.36" data-path="Ali_2020_jfe.html">
            
                <a href="Ali_2020_jfe.html">
            
                    
                    Shared Analyst Coverage: Unifying Momentum Spillover Effects
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.37" data-path="q-factor.html">
            
                <a href="q-factor.html">
            
                    
                    Q-Factor Model
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.38" data-path="DHS-4-factors.html">
            
                <a href="DHS-4-factors.html">
            
                    
                    Short- and Long-Horizon Behavioral Factors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.39" data-path="Stambaugh-Yuan-2017.html">
            
                <a href="Stambaugh-Yuan-2017.html">
            
                    
                    Stambaugh-Yuan Four Factors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.40" data-path="cn_nlp.html">
            
                <a href="cn_nlp.html">
            
                    
                    Chinese NLP
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.41" data-path="cn_shell.html">
            
                <a href="cn_shell.html">
            
                    
                    Chinese Stock Market Shell Value
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.42" data-path="cn_ff3.html">
            
                <a href="cn_ff3.html">
            
                    
                    Size and Value
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.43" data-path="retail.html">
            
                <a href="retail.html">
            
                    
                    Tracking Retail Investor Activity
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4.44" data-path="GCN.html">
            
                <a href="GCN.html">
            
                    
                    Graph Convolution and Graph Attention
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.45" data-path="phack.html">
            
                <a href="phack.html">
            
                    
                    P-hacking
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.46" data-path="general_tech_dl.html">
            
                <a href="general_tech_dl.html">
            
                    
                    General Structures and Techniques in Deep Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.47" data-path="TCN.html">
            
                <a href="TCN.html">
            
                    
                    Temporal Convolutional Network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.48" data-path="whichbeta.html">
            
                <a href="whichbeta.html">
            
                    
                    Which Beta?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.49" data-path="OOS.html">
            
                <a href="OOS.html">
            
                    
                    Out of Sample Predictability
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.50" data-path="FF3_cryoto.html">
            
                <a href="FF3_cryoto.html">
            
                    
                    Multi-factor in Cryptocurrency
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.51" data-path="quant_ml.html">
            
                <a href="quant_ml.html">
            
                    
                    Quant Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.52" data-path="behave.html">
            
                <a href="behave.html">
            
                    
                    Behavioral Finance
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.53" data-path="d-reduct.html">
            
                <a href="d-reduct.html">
            
                    
                    Nonlinear Dimension Reduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.54" data-path="moment.html">
            
                <a href="moment.html">
            
                    
                    Momentum
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.55" data-path="miscell.html">
            
                <a href="miscell.html">
            
                    
                    Lottery-like Stocks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.56" data-path="tabnet.html">
            
                <a href="tabnet.html">
            
                    
                    TabNet
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.57" data-path="kaggle-fin.html">
            
                <a href="kaggle-fin.html">
            
                    
                    Kaggle Finance Competition Solution Summary
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.58" data-path="feature_int.html">
            
                <a href="feature_int.html">
            
                    
                    Feature Interaction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.59" data-path="ESG.html">
            
                <a href="ESG.html">
            
                    
                    ESG
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.60" data-path="ivol.html">
            
                <a href="ivol.html">
            
                    
                    Idiosyncratic Volatility
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.61" data-path="volume.html">
            
                <a href="volume.html">
            
                    
                    Volume
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.62" data-path="overnight.html">
            
                <a href="overnight.html">
            
                    
                    Overnight Return Reserval
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.63" data-path="NAS.html">
            
                <a href="NAS.html">
            
                    
                    Neural Architecture Search
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.64" data-path="meta.html">
            
                <a href="meta.html">
            
                    
                    Meta Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.65" data-path="nominal_illusion.html">
            
                <a href="nominal_illusion.html">
            
                    
                    Nominal Price Ilusion
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.66" data-path="co-trade.html">
            
                <a href="co-trade.html">
            
                    
                    Co-trade Network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.67" data-path="COI_flow_decomp.html">
            
                <a href="COI_flow_decomp.html">
            
                    
                    Order Imbalance with Trade Flow Decomposition
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.68" data-path="mlts.html">
            
                <a href="mlts.html">
            
                    
                    Time Seris Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.69" data-path="PEAD.html">
            
                <a href="PEAD.html">
            
                    
                    PEAD
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.70" data-path="risk_parity.html">
            
                <a href="risk_parity.html">
            
                    
                    Risk Parity
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.71" data-path="nlp_fin.html">
            
                <a href="nlp_fin.html">
            
                    
                    NLP in Finance
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.72" data-path="self_super_tab.html">
            
                <a href="self_super_tab.html">
            
                    
                    Self-supervised Learning for Tabular Data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.73" data-path="regime.html">
            
                <a href="regime.html">
            
                    
                    Regime Modeling
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.74" data-path="fin_stat.html">
            
                <a href="fin_stat.html">
            
                    
                    Financial Statement Related
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.75" data-path="dl_fin.html">
            
                <a href="dl_fin.html">
            
                    
                    Deep Learning in Finance
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Graph Convolution and Graph Attention</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon "></span><a href="#graph-convolution-network-gcn-and-graph-attention-network-gat"><b> </b>Graph Convolution Network (GCN) and Graph Attention Network (GAT)</a></li><ul><li><span class="title-icon "></span><a href="#laplacian-matrix-on-graph"><b>1. </b>Laplacian Matrix on Graph</a></li><li><span class="title-icon "></span><a href="#fourier-transformer"><b>2. </b>Fourier Transformer</a></li><li><span class="title-icon "></span><a href="#convolution-on-graphs"><b>3. </b>Convolution on Graphs</a></li><li><span class="title-icon "></span><a href="#fast-localized-spectral-filtering"><b>4. </b>Fast Localized Spectral Filtering</a></li><li><span class="title-icon "></span><a href="#fast-approximate-convolutions-on-graphs"><b>5. </b>Fast Approximate Convolutions on Graphs</a></li><li><span class="title-icon "></span><a href="#simple-gcn"><b>6. </b>Simple GCN</a></li><li><span class="title-icon "></span><a href="#graphsage"><b>7. </b>GraphSAGE</a></li><li><span class="title-icon "></span><a href="#graph-attention-network"><b>8. </b>Graph Attention Network</a></li></ul></ul></div><a href="#graph-convolution-network-gcn-and-graph-attention-network-gat" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><h1 id="graph-convolution-network-gcn-and-graph-attention-network-gat"><a name="graph-convolution-network-gcn-and-graph-attention-network-gat" class="anchor-navigation-ex-anchor" href="#graph-convolution-network-gcn-and-graph-attention-network-gat"><i class="fa fa-link" aria-hidden="true"></i></a> Graph Convolution Network (GCN) and Graph Attention Network (GAT)</h1>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/120311352" target="_blank">Introduction to GCN</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/62750137" target="_blank">GraphSAGE</a></li>
<li><a href="https://proceedings.neurips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html" target="_blank">Defferrard, Micha&#xEB;l, Xavier Bresson, and Pierre Vandergheynst. &quot;Convolutional neural networks on graphs with fast localized spectral filtering.&quot; Advances in neural information processing systems 29 (2016).</a></li>
<li><a href="https://arxiv.org/abs/1609.02907" target="_blank">Kipf, Thomas N., and Max Welling. &quot;Semi-supervised classification with graph convolutional networks.&quot; International Conference on Learning Representations (2017).</a></li>
<li><a href="https://proceedings.neurips.cc/paper/6703-inductive-representation-learning-on-large-graphs" target="_blank">Hamilton, Will, Zhitao Ying, and Jure Leskovec. &quot;Inductive representation learning on large graphs.&quot; Advances in neural information processing systems 30 (2017).</a></li>
<li><a href="https://arxiv.org/pdf/1902.07153.pdf" target="_blank">Wu, Felix, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. &quot;Simplifying graph convolutional networks.&quot; In <em>International conference on machine learning</em>, pp. 6861-6871. PMLR, 2019.</a></li>
<li><a href="https://arxiv.org/abs/1710.10903" target="_blank">Veli&#x10D;kovi&#x107;, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. &quot;Graph attention networks.&quot; International Conference on Learning Representations (2018).</a></li>
</ul>
<h2 id="laplacian-matrix-on-graph"><a name="laplacian-matrix-on-graph" class="anchor-navigation-ex-anchor" href="#laplacian-matrix-on-graph"><i class="fa fa-link" aria-hidden="true"></i></a>1. Laplacian Matrix on Graph</h2>
<p>The Laplacian matrix of a <script type="math/tex; ">n</script>-vertex graph <script type="math/tex; ">\mathcal{G}</script> is defined as
<script type="math/tex; mode=display">
L = D - W,
</script>
where <script type="math/tex; ">W \in \mathbb{R}^{n \times n}</script> is the adjacent matrix with weights <script type="math/tex; ">W_{ij}</script> and <script type="math/tex; ">D = \mathrm{diag}(D_i) \in \mathbb{R}^{n \times n}, D_i = \sum_{j}W_{ij}</script> is the degree matrix. The normalized Laplacian matrix is defined as
<script type="math/tex; mode=display">
L_{N}=D^{-1 / 2}(D-W) D^{-1 / 2} = I_n - D^{-1 / 2} W D^{-1 / 2}.
</script>
The Laplace operator is defined as
<script type="math/tex; mode=display">
\Delta f=\sum_{i=1}^{n} \frac{\partial^{2} f}{\partial x_{i}^{2}},
</script>
In <script type="math/tex; ">1</script>-dimensional space, we have
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial^{2} f}{\partial x_{i}^{2}} =f^{\prime \prime}(x) 
& \approx f^{\prime}(x)-f^{\prime}(x-1) \\
& \approx f(x+1)-f(x)-(f(x)-f(x-1)) \\
&=f(x+1)+f(x-1))-2 f(x).
\end{aligned}
</script>
Similarly, if we consider a <script type="math/tex; ">n</script>-vertex graph <script type="math/tex; ">\mathcal{G}</script> and a function <script type="math/tex; ">\mathbf{f}</script> on it,
<script type="math/tex; mode=display">
\mathbf{f} = (f_1, \ldots, f_n),
</script>
where <script type="math/tex; ">f_i</script> is the function value of the <script type="math/tex; ">i</script>-th vertex in the graph. Then the Laplacian operation on this function <script type="math/tex; ">\mathbf{f}</script> on graph <script type="math/tex; ">\mathcal{G}</script> should be
<script type="math/tex; mode=display">
\begin{aligned}
\Delta f_{i} =\sum_{j \in N_{i}} \frac{\partial f_{i}}{\partial j^{2}} & \approx \sum_{j} W_{i j}\left(f_{i}-f_{j}\right) \\
&=\sum_{j} W_{i j}\left(f_{i}-f_{j}\right) \\
&=\left(\sum_{j} W_{i j}\right) f_{i}-\sum_{j} W_{i j} f_{j} \\
&=D_{i} f_{i}-\sum_{j} W_{i j} f_{j}.
\end{aligned}
</script>
In a vector form, we can recover the definition of Laplacian matrix on graph <script type="math/tex; ">\mathcal{G}</script>,
<script type="math/tex; mode=display">
\Delta \mathbf{f} = (D - W) \mathbf{f} = L \mathbf{f}.
</script>
For an undirected graph, the Laplacian matrix is a real symmetric matrix and therefore we have the spectral decomposition
<script type="math/tex; mode=display">
L = U \Lambda U^\top, \;\; \Lambda = \mathrm{diag}(\lambda_1, \ldots, \lambda_n).
</script>
There are also some properties for Laplacian matrix <script type="math/tex; ">L</script>, </p>
<ul>
<li>There is at least one eigenvalue 0.</li>
<li>All the eigenvalues are non-negative. For normalized Laplacian matrix, all the eigenvalues are between 0 and 2, and the summation is <script type="math/tex; ">n</script>.</li>
<li>If there are <script type="math/tex; ">k</script> eigenvalues with value 0, then there are <script type="math/tex; ">k</script> mutually unconnected sub-graphs within the graph.</li>
</ul>
<h2 id="fourier-transformer"><a name="fourier-transformer" class="anchor-navigation-ex-anchor" href="#fourier-transformer"><i class="fa fa-link" aria-hidden="true"></i></a>2. Fourier Transformer</h2>
<p>Recall Fourier transformer of function <script type="math/tex; ">f(\cdot)</script>,
<script type="math/tex; mode=display">
\mathcal{F}_{T}(\omega)=\int_{-\infty}^{+\infty} f(t) e^{-i \omega t} d t,
</script>
where <script type="math/tex; ">\{e^{-iwt}\}</script> is a set of orthogonal basis. We also have the inverse Fourier transformer,
<script type="math/tex; mode=display">
f(t)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty} \mathcal{F}_{T}(\omega) e^{i \omega t} d \omega.
</script></p>
<p>For the basis function <script type="math/tex; ">\{e^{-iwt}\}</script>, notice that it is the eigenfunction for Laplacian operator <script type="math/tex; ">\Delta</script>,
<script type="math/tex; mode=display">
\Delta e^{-i \omega t}=\nabla^{2} e^{-i \omega t}=\frac{d e^{-i \omega t}}{d t^{2}}=-\omega^{2} e^{-i \omega t}.
</script>
Similarly, the eigenvectors <script type="math/tex; ">U = [\mathbf{u}_1, \ldots, \mathbf{u}_n]</script> of graph Laplacian matrix should also be the basis for the Fourier transformer on the graph <script type="math/tex; ">\mathcal{G}</script>, i.e.,
<script type="math/tex; mode=display">
\mathcal{F}_T\left(\lambda_{k}\right) \triangleq \hat{f}_{k}= \langle \mathbf{f}, \mathbf{u}_k \rangle = \sum_{i=1}^{n} f_i \mathbf{u}_{k}(i).
</script>
Therefore,
<script type="math/tex; mode=display">
\hat{\mathbf{f}} = U^\top \mathbf{f}.
</script>
Similarly, we have the inverse Fourier transformer on graph <script type="math/tex; ">\mathcal{G}</script>,
<script type="math/tex; mode=display">
\mathbf{f} = U \hat{\mathbf{f}} = U U^\top \mathbf{f}.
</script></p>
<h2 id="convolution-on-graphs"><a name="convolution-on-graphs" class="anchor-navigation-ex-anchor" href="#convolution-on-graphs"><i class="fa fa-link" aria-hidden="true"></i></a>3. Convolution on Graphs</h2>
<p>Denote the continuous convolution for two functions <script type="math/tex; ">f, g</script>,
<script type="math/tex; mode=display">
(f * g)(t) \stackrel{\text { def }}{=} \int_{\mathbb{R}^{n}} f(\tau) g(t-\tau) d \tau,
</script>
as well as the discrete convolution,
<script type="math/tex; mode=display">
(f * g)(n)=\sum_{m=-\infty}^{\infty} f(m) g(n-m)=\sum_{m=-\infty}^{\infty} f(n-m) g(m).
</script>
Recall the property for convolution and Fourier transformer,
<script type="math/tex; mode=display">
\mathcal{F}\{f * g\}=\mathcal{F}\{f\} \cdot \mathcal{F}\{g\}.
</script>
Therefore,
<script type="math/tex; mode=display">
f * g=\mathcal{F}^{-1} \left(\mathcal{F}\{f\} \cdot \mathcal{F}\{g\} \right).
</script>
For convolution operations on graph <script type="math/tex; ">\mathcal{G}</script>, similarly, we have
<script type="math/tex; mode=display">
(\mathbf{f} * \mathbf{g})_{\mathcal{G}} =\mathcal{F}^{-1}[\mathcal{F}\{\mathbf{f}\} \cdot \mathcal{F}\{\mathbf{g}\}] =\mathcal{F}^{-1}\left[U^{T} \mathbf{f} \cdot \hat{\mathbf{g}}\right] = \mathcal{F}^{-1}\left[\mathrm{diag}(\hat{g}_1, \ldots, \hat{g}_n) U^{T} \mathbf{f} \right].
</script>
If we parameterize <script type="math/tex; ">\hat{\mathbf{g}}</script> directly by trainable parameters <script type="math/tex; ">\theta_1, \ldots, \theta_n</script>, 
<script type="math/tex; mode=display">
(\mathbf{f} * \mathbf{g})_{\mathcal{G}} = U \mathrm{diag}(\theta_1, \ldots, \theta_n) U^{T} \mathbf{f}.
</script>
This is the convolution filter on graphs.</p>
<h2 id="fast-localized-spectral-filtering"><a name="fast-localized-spectral-filtering" class="anchor-navigation-ex-anchor" href="#fast-localized-spectral-filtering"><i class="fa fa-link" aria-hidden="true"></i></a>4. Fast Localized Spectral Filtering</h2>
<ul>
<li><a href="https://proceedings.neurips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html" target="_blank">Defferrard, Micha&#xEB;l, Xavier Bresson, and Pierre Vandergheynst. &quot;Convolutional neural networks on graphs with fast localized spectral filtering.&quot; Advances in neural information processing systems 29 (2016).</a></li>
</ul>
<p>Recall that in the above graph convolution, <script type="math/tex; ">\hat{\mathbf{g}}</script> is a vector depending on the eigenvalues of the Laplacian matrix <script type="math/tex; ">L</script>, we can rewrite it as
<script type="math/tex; mode=display">
\hat{\mathbf{g}} = g_\theta(\Lambda),
</script>
and the graph convolution is then
<script type="math/tex; mode=display">
\mathbf{y} = U g_\theta(\Lambda) U^{T} \mathbf{x}.
</script>
Notice that it can also be seen as a signal <script type="math/tex; ">\mathbf{x}</script> filtered by <script type="math/tex; ">g_\theta</script> as,
<script type="math/tex; mode=display">
\mathbf{y} = g_\theta(L) \mathbf{x} = g_\theta(U \Lambda U^\top) \mathbf{x} = U g_\theta(\Lambda) U^{T} \mathbf{x}.
</script>
The parameterization in the previous section can be seen as a non-parametric filter, i.e.,
<script type="math/tex; mode=display">
g_\theta(\Lambda) = \mathrm{diag}(\theta_1, \ldots, \theta_n).
</script>
There are however two limitations with non-parametric filters: </p>
<ul>
<li>they are not localized in space</li>
<li>their learning complexity is in <script type="math/tex; ">\mathcal{O}(n)</script>, the dimensionality of the graph. </li>
</ul>
<p>These issues can be overcome with the use of a polynomial filter,
<script type="math/tex; mode=display">
g_\theta(\Lambda) = \sum_{k=0}^{K-1} \theta_{k} \Lambda^{k}.
</script>
Under the polynomial filter, the convolution is given by 
<script type="math/tex; mode=display">
\mathbf{y} = U g_\theta(\Lambda) U^\top \mathbf{x} = \left(\sum_{k=0}^{K-1} \theta_{k} L^{k}\right) \mathbf{x}.
</script>
Notice that <script type="math/tex; ">d_{\mathcal{G}}(i, j)>K</script> implies <script type="math/tex; ">\left(L^{K}\right)_{i, j}=0</script>, where <script type="math/tex; ">d_{\mathcal{G}}</script> is the shortest path distance between <script type="math/tex; ">i</script> and <script type="math/tex; ">j</script>. Consequently, spectral filters represented by <script type="math/tex; ">K^{\text {th }}</script> order polynomials of the Laplacian matrix are exactly <script type="math/tex; ">K</script>-localized. Besides, their learning complexity is <script type="math/tex; ">\mathcal{O}(K)</script>, the support size of the filter, and thus the same complexity as classical CNNs.</p>
<p>However, the matrix computation of <script type="math/tex; ">U</script> matrix part still involves the <script type="math/tex; ">\mathcal{O}(n^2)</script> computation complexity. Recall that the Chebyshev polynomial <script type="math/tex; ">T_{k}(x)</script> of order <script type="math/tex; ">k</script> may be computed by the stable recurrence relation 
<script type="math/tex; mode=display">
T_{k}(x)=2 x T_{k-1}(x)-T_{k-2}(x),
</script>
with <script type="math/tex; ">T_{0}=1</script> and <script type="math/tex; ">T_{1}=x</script>. These polynomials form an orthogonal basis for <script type="math/tex; ">L^{2}\left([-1,1], d y / \sqrt{1-y^{2}}\right)</script>, the Hilbert space of square integrable functions with respect to the measure <script type="math/tex; ">d y / \sqrt{1-y^{2}}</script>. A filter can thus be parametrized as the truncated expansion,
<script type="math/tex; mode=display">
g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} T_{k}(\widetilde{\Lambda}),
</script>
where 
<script type="math/tex; mode=display">
\widetilde{\Lambda}=2 \Lambda / \lambda_{\max }-I_{n}
</script>
is the normalized diagonal matrix with scaled eigenvalues that lie in <script type="math/tex; ">[-1,1]</script>. The convolution operation is therefore,
<script type="math/tex; mode=display">
\mathbf{y} = \sum_{k=0}^{K-1} \theta_{k} T_{k}(\widetilde{L}) \mathbf{x}.
</script>
Here <script type="math/tex; ">\widetilde{L}=2 L / \lambda_{\max }-I_{n}</script>. Denoting <script type="math/tex; ">\bar{\mathbf{x}}_{k}=T_{k}(\widetilde{L}) \mathbf{x} \in \mathbb{R}^{n}</script>, we can use the recurrence relation to compute <script type="math/tex; ">\bar{\mathbf{x}}_{k}=2 \widetilde{L} \bar{\mathbf{x}}_{k-1}-\bar{\mathbf{x}}_{k-2}</script> with <script type="math/tex; ">\bar{\mathbf{x}}_{0}=\mathbf{x}</script> and <script type="math/tex; ">\bar{\mathbf{x}}_{1}=\widetilde{L} \mathbf{x}</script>. The entire filtering operation then costs <script type="math/tex; ">\mathcal{O}(K|\mathcal{E}|)</script> operations, where <script type="math/tex; ">|\mathcal{E}|</script> is the number of edges in the graph <script type="math/tex; ">\mathcal{G}</script>.</p>
<h2 id="fast-approximate-convolutions-on-graphs"><a name="fast-approximate-convolutions-on-graphs" class="anchor-navigation-ex-anchor" href="#fast-approximate-convolutions-on-graphs"><i class="fa fa-link" aria-hidden="true"></i></a>5. Fast Approximate Convolutions on Graphs</h2>
<ul>
<li><a href="https://arxiv.org/abs/1609.02907" target="_blank">Kipf, Thomas N., and Max Welling. &quot;Semi-supervised classification with graph convolutional networks.&quot; International Conference on Learning Representations (2017).</a></li>
</ul>
<p>Recall that in the previous section, we have the convolution operation on vector <script type="math/tex; ">\mathbf{x}</script> of a graph <script type="math/tex; ">\mathcal{G}</script> using Chebyshev polynomials,
<script type="math/tex; mode=display">
\mathbf{y} = \sum_{k=0}^{K-1} \theta_{k} T_{k}(\widetilde{L}) \mathbf{x}.
</script>
If we consider the simple case of <script type="math/tex; ">K=2</script> and consider the normalized Laplacian matrix <script type="math/tex; ">L_N</script>, and approximate <script type="math/tex; ">\lambda_{\max}=2</script>, which is the eigenvalue upper bound for <script type="math/tex; ">L_N</script>, we have
<script type="math/tex; mode=display">
\mathbf{y} = \sum_{k=0}^{1} \theta_{k} T_{k}(\widetilde{L}_N) \mathbf{x} = \theta_{0} \mathbf{x}+\theta_{1}\left(L_N-I_{n}\right) \mathbf{x}=\theta_{0} \mathbf{x}-\theta_{1} D^{-\frac{1}{2}} W D^{-\frac{1}{2}} \mathbf{x}.
</script>
In practice, it can be beneficial to constrain the number of parameters further to address overfitting and to minimize the number of operations (such as matrix multiplications) per layer. This leaves us with the following expression using <script type="math/tex; ">\theta = \theta_0 = - \theta_1</script>,
<script type="math/tex; mode=display">
\mathbf{y} = \theta \mathbf{x} + \theta D^{-\frac{1}{2}} W D^{-\frac{1}{2}} \mathbf{x} = \theta \left(I_n + D^{-\frac{1}{2}} W D^{-\frac{1}{2}} \right) \mathbf{x}.
</script>
Note that <script type="math/tex; ">I_{n}+D^{-\frac{1}{2}} W D^{-\frac{1}{2}}</script> now has eigenvalues in the range <script type="math/tex; ">[0,2]</script>. Repeated application of this operator can therefore lead to numerical instabilities and exploding/vanishing gradients when used in a deep neural network model. </p>
<p>To alleviate this problem, we introduce the following renormalization trick: 
<script type="math/tex; mode=display">
I_{n}+D^{-\frac{1}{2}} W D^{-\frac{1}{2}} \rightarrow \widetilde{D}^{-\frac{1}{2}} \widetilde{W} \widetilde{D}^{-\frac{1}{2}},
</script>
with <script type="math/tex; ">\widetilde{W}=W+I_{N}</script> and <script type="math/tex; ">\widetilde{D}_{i i}=\sum_{j} \widetilde{W}_{i j}</script>.</p>
<p>We can generalize this definition to a signal <script type="math/tex; ">X \in \mathbb{R}^{N \times C}</script> with <script type="math/tex; ">C</script> input channels (i.e. a <script type="math/tex; ">C</script>-dimensional feature vector for every node) and <script type="math/tex; ">F</script> filters or feature maps as follows,
<script type="math/tex; mode=display">
Y=\tilde{D}^{-\frac{1}{2}} \tilde{W} \tilde{D}^{-\frac{1}{2}} X \Theta,
</script>
where <script type="math/tex; ">\Theta \in \mathbb{R}^{C \times F}</script> is now a matrix of filter parameters and <script type="math/tex; ">Y \in \mathbb{R}^{N \times F}</script> is the convolved signal matrix. This filtering operation has complexity <script type="math/tex; ">\mathcal{O}(|\mathcal{E}| F C)</script>, as <script type="math/tex; ">\tilde{W} X</script> can be efficiently implemented as a product of a sparse matrix with a dense matrix.</p>
<h2 id="simple-gcn"><a name="simple-gcn" class="anchor-navigation-ex-anchor" href="#simple-gcn"><i class="fa fa-link" aria-hidden="true"></i></a>6. Simple GCN</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1902.07153.pdf" target="_blank">Wu, Felix, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. &quot;Simplifying graph convolutional networks.&quot; In <em>International conference on machine learning</em>, pp. 6861-6871. PMLR, 2019.</a></li>
</ul>
<p>From Kipf and Welling (2017) above, for a input <script type="math/tex; ">X^{(t)} \in \mathbb{R}^{N \times C}</script> with <script type="math/tex; ">C</script> input channels (i.e. a <script type="math/tex; ">C</script>-dimensional feature vector for every node) and <script type="math/tex; ">F</script> filters or feature maps as follows,
<script type="math/tex; mode=display">
X^{(t+1)}= \sigma\left(S X^{(t)} \Theta \right), \; \; S = \tilde{D}^{-\frac{1}{2}} \tilde{W} \tilde{D}^{-\frac{1}{2}}.
</script>
where <script type="math/tex; ">\Theta \in \mathbb{R}^{C \times F}</script> is now a matrix of filter parameters, <script type="math/tex; ">\sigma</script> is the non-liner activation function and <script type="math/tex; ">X^{(t+1)} \in \mathbb{R}^{N \times F}</script> is the convolved signal matrix. For a <script type="math/tex; ">K</script>-depth network, we will repeat the above procedure <script type="math/tex; ">K</script> times. </p>
<p>This paper hypothesizes that the nonlinearity between GCN layers is not critical - but that the majority of the benefit arises from the local averaging. The resulting model is linear, but still has the same increased &#x201C;receptive field&#x201D; of a <script type="math/tex; ">K</script>-layer GCN, i.e.,
<script type="math/tex; mode=display">
Y =\operatorname{softmax}\left(S \ldots S S X \Theta^{(1)} \Theta^{(2)} \ldots \Theta^{(K)}\right) = \operatorname{softmax} \left(S^K X \Theta\right).
</script>
Under this network, the authors obtain a comparable experiment results with the previous structure, and the computation cost / the number of parameters is significantly reduced. Notice that <script type="math/tex; ">S</script> is a <script type="math/tex; ">2 |\mathcal{E}|</script>-sparse matrix, where <script type="math/tex; ">|\mathcal{E}|</script> is the number of edges in the graph <script type="math/tex; ">\mathcal{G}</script>, and the exponential computation for sparse matrix is quite fast.</p>
<h2 id="graphsage"><a name="graphsage" class="anchor-navigation-ex-anchor" href="#graphsage"><i class="fa fa-link" aria-hidden="true"></i></a>7. GraphSAGE</h2>
<ul>
<li><a href="https://proceedings.neurips.cc/paper/6703-inductive-representation-learning-on-large-graphs" target="_blank">Hamilton, Will, Zhitao Ying, and Jure Leskovec. &quot;Inductive representation learning on large graphs.&quot; Advances in neural information processing systems 30 (2017).</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/62750137" target="_blank">GraphSAGE</a></li>
</ul>
<p>We now consider the graph convolution from a different aspect. We mimic the logic of convolution on graph: for a 3x3 convolution layer, we conduct weighted average of the value of the 1-step neighbors of the center point. The next layer convolution operation increases the &quot;receptive field&quot; to the 2-step neighbors of the center point.</p>
<p>The algorithm is given below. For each step <script type="math/tex; ">k=1, \ldots, K</script>, we first gather and aggregate all the hidden vecotr from previous step <script type="math/tex; ">\mathbf{h}_{u}^{k-1}</script> in the neightbourhood of vertex <script type="math/tex; ">v</script>, i.e., <script type="math/tex; ">u \in \mathcal{N}(v)</script>. Then in line 5, we combine the neighborhood information vector <script type="math/tex; ">\mathbf{v}_{\mathcal{N}(v)}^k</script> with <script type="math/tex; ">\mathbf{h}_{v}^{k-1}</script> of <script type="math/tex; ">v</script>. </p>
<p>This process has been repeated <script type="math/tex; ">K</script> times and the information from all the <script type="math/tex; ">K</script>-step neighbor will be included.  Instead of training a distinct embedding vector for each node, this algorithm train a set of <em>aggregator functions</em> that learn to aggregate feature information from a node&#x2019;s local neighborhood.</p>
<p>There are several choices of aggregator function:</p>
<ul>
<li><p><strong>Mean aggregator</strong>: which is the average of all the <script type="math/tex; ">\left\{\mathbf{h}_{u}^{k-1}, \forall u \in \mathcal{N}(v)\right\}</script>.</p>
</li>
<li><p><strong>Pooling aggregator</strong>: each neighbor&#x2019;s vector is independently fed through a fully-connected neural network; following this transformation, an elementwise max-pooling operation is applied to aggregate information across the neighbor set:
<script type="math/tex; mode=display">
  \text { AGGREGATE }_{k}^{\text {pool }}=\max \left(\left\{\sigma\left(W \mathbf{h}_{u}^{k}+\mathbf{b}\right), \forall u \in \mathcal{N}(v)\right\}\right).
  </script></p>
</li>
</ul>
<p><img src="pic/graphsage.png" alt=""></p>
<h2 id="graph-attention-network"><a name="graph-attention-network" class="anchor-navigation-ex-anchor" href="#graph-attention-network"><i class="fa fa-link" aria-hidden="true"></i></a>8. Graph Attention Network</h2>
<ul>
<li><a href="https://arxiv.org/abs/1710.10903" target="_blank">Veli&#x10D;kovi&#x107;, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. &quot;Graph attention networks.&quot; International Conference on Learning Representations (2018).</a></li>
</ul>
<p>The input to our layer is a set of node features, <script type="math/tex; ">\mathbf{h}=\left\{\vec{h}_{1}, \vec{h}_{2}, \ldots, \vec{h}_{N}\right\}, \vec{h}_{i} \in \mathbb{R}^F</script>, where <script type="math/tex; ">N</script> is the number of nodes, and <script type="math/tex; ">F</script> is the number of features in each node. The layer produces a new set of node features (of potentially different cardinality <script type="math/tex; ">F^{\prime}</script> ), <script type="math/tex; ">\mathbf{h}^{\prime}=\left\{\vec{h}_{1}^{\prime}, \vec{h}_{2}^{\prime}, \ldots, \vec{h}_{N}^{\prime}\right\}, \vec{h}_{i}^{\prime} \in \mathbb{R}^{F^{\prime}}</script>, as its output.</p>
<p>Consider a self-attention mechanism on the nodes
<script type="math/tex; mode=display">
e_{i j}=a\left(W \vec{h}_{i}, W \vec{h}_{j}\right),
</script>
where <script type="math/tex; ">W \in \mathbb{R}^{F^\prime \times F}</script> is the weight matrix and <script type="math/tex; ">a</script> is the attention mechanism <script type="math/tex; ">a: \mathbb{R}^{F^{\prime}} \times \mathbb{R}^{F^{\prime}} \rightarrow \mathbb{R}</script>. </p>
<p>Consider the masked attention softmax, which normalize <script type="math/tex; ">e_{ij}</script> across all the neighbors of node <script type="math/tex; ">i</script>,
<script type="math/tex; mode=display">
\alpha_{i j}=\operatorname{softmax}_{j}\left(e_{i j}\right)=\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(e_{i k}\right)}.
</script>
We consider the attention mechanism <script type="math/tex; ">a</script> as a single-layer feed-forward neural network, parametrized by a weight vector <script type="math/tex; ">\overrightarrow{\mathbf{a}} \in \mathbb{R}^{2 F^{\prime}}</script>, and applying the LeakyReLU non-linearity,
<script type="math/tex; mode=display">
\alpha_{i j}=\frac{\exp \left(\operatorname{LeakyReLU}\left(\overrightarrow{\mathbf{a}}^{T}\left[W \vec{h}_{i} \| W \vec{h}_{j}\right]\right)\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(\operatorname{LeakyReLU}\left(\overrightarrow{\mathbf{a}}^{T}\left[W \vec{h}_{i} \| W \vec{h}_{k}\right]\right)\right)}.
</script>
where <script type="math/tex; ">\|</script> is the concatenation operation. Therefore, the one-layer graph attention is computed as follows,
<script type="math/tex; mode=display">
\vec{h}_{i}^{\prime}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} W \vec{h}_{j}\right).
</script>
Utilizing the multi-head attention mechanism, we can extend the network structure to
<script type="math/tex; mode=display">
\vec{h}_{i}^{\prime}=\|_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} W^{k} \vec{h}_{j}\right),
</script>
where <script type="math/tex; ">\|</script> represents concatenation, <script type="math/tex; ">\alpha_{i j}^{k}</script> are normalized attention coefficients computed by the <script type="math/tex; ">k</script>-th attention mechanism <script type="math/tex; ">\left(a^{k}\right)</script>, and <script type="math/tex; ">W^{k}</script> is the corresponding input linear transformation&apos;s weight matrix. Note that, in this setting, the final returned output, <script type="math/tex; ">\vec{h}_i^{\prime} \in \mathbb{R}^{K F^{\prime}}</script>.</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="retail.html" class="navigation navigation-prev " aria-label="Previous page: Tracking Retail Investor Activity">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="phack.html" class="navigation navigation-next " aria-label="Next page: P-hacking">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Graph Convolution and Graph Attention","level":"1.4.44","depth":2,"next":{"title":"P-hacking","level":"1.4.45","depth":2,"path":"notes/phack.md","ref":"./notes/phack.md","articles":[]},"previous":{"title":"Tracking Retail Investor Activity","level":"1.4.43","depth":2,"path":"notes/retail.md","ref":"./notes/retail.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax","image-captions","github","anchors","anchor-navigation-ex","-sharing","sharing-plus@^0.0.2","github-buttons","embed-pdf","livereload"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"github":{"url":"https://github.com/ZizouHe/"},"livereload":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"fa fa-hand-o-right","level2Icon":"fa fa-hand-o-right","level3Icon":"fa fa-hand-o-right","showLevelIcon":false},"mode":"float","multipleH1":false,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":true},"github-buttons":{"buttons":[{"user":"ZizouHe","repo":"reading-notes","type":"star","size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"qq":true,"all":["facebook","google","twitter","weibo","qq","linkedin"],"douban":false,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":false,"messenger":false,"line":false,"vk":false,"pocket":false,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"anchors":{},"embed-pdf":{},"image-captions":{"caption":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","align":"right","variable_name":"_pictures"}},"theme":"default","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{"_pictures":[{"backlink":"./notes/RDN.html#fig1.4.2.1","level":"1.4.2","align":"right","list_caption":"Figure: Example (a) data graph and (b) model graph.","alt":"Example (a) data graph and (b) model graph.","nro":1,"url":"./pic/RDN-1.png","index":1,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Example (a) data graph and (b) model graph.","attributes":{},"skip":false,"key":"1.4.2.1"},{"backlink":"./notes/RDN.html#fig1.4.2.2","level":"1.4.2","align":"right","list_caption":"Figure: RDN inference algorithm.","alt":"RDN inference algorithm.","nro":2,"url":"./pic/RDN-2.png","index":2,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"RDN inference algorithm.","attributes":{},"skip":false,"key":"1.4.2.2"},{"backlink":"./notes/Faster_Frank_Wolfe.html#fig1.4.14.1","level":"1.4.14","align":"right","list_caption":"Figure: Frank-Wolfe Algorithm","alt":"Frank-Wolfe Algorithm","nro":3,"url":"./pic/Frank-Wolfe.png","index":1,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Frank-Wolfe Algorithm","attributes":{},"skip":false,"key":"1.4.14.1"},{"backlink":"./notes/Fin_Fan_Book.html#fig1.4.15.1","level":"1.4.15","align":"right","list_caption":"Figure: Efficient Frontier with risk-free asset","alt":"Efficient Frontier with risk-free asset","nro":4,"url":"./pic/Eff_Front_1.png","index":1,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Efficient Frontier with risk-free asset","attributes":{},"skip":false,"key":"1.4.15.1"},{"backlink":"./notes/Fin_Fan_Book.html#fig1.4.15.2","level":"1.4.15","align":"right","list_caption":"Figure: Efficient Frontier without risk-free asset","alt":"Efficient Frontier without risk-free asset","nro":5,"url":"./pic/Eff_Front_2.png","index":2,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Efficient Frontier without risk-free asset","attributes":{},"skip":false,"key":"1.4.15.2"}]},"title":"Reading List & Notes","gitbook":"*"},"file":{"path":"notes/GCN.md","mtime":"2022-10-26T16:39:28.913Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2024-06-30T07:07:50.880Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

