
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Reinforecement Learning: An Introduction Â· Reading List & Notes</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-image-captions/image-captions.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchors/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    
    <link rel="prev" href="Double_ML.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"ZizouHe","repo":"reading-notes","type":"star","size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../contents/already.html">
            
                <a href="../contents/already.html">
            
                    
                    Finished List
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../contents/bayes.html">
            
                <a href="../contents/bayes.html">
            
                    
                    Bayesian Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../contents/DL.html">
            
                <a href="../contents/DL.html">
            
                    
                    Deep Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../contents/CS.html">
            
                <a href="../contents/CS.html">
            
                    
                    General CS
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../contents/DS.html">
            
                <a href="../contents/DS.html">
            
                    
                    General Data Science
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="../contents/ML.html">
            
                <a href="../contents/ML.html">
            
                    
                    Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../contents/opt.html">
            
                <a href="../contents/opt.html">
            
                    
                    Numerical and Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../contents/stat.html">
            
                <a href="../contents/stat.html">
            
                    
                    Traditional Statistics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="../contents/book.html">
            
                <a href="../contents/book.html">
            
                    
                    Books
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../contents/reading.html">
            
                <a href="../contents/reading.html">
            
                    
                    Reading List
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../contents/bayesing.html">
            
                <a href="../contents/bayesing.html">
            
                    
                    Bayesian Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../contents/DLing.html">
            
                <a href="../contents/DLing.html">
            
                    
                    Deep Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="../contents/MLing.html">
            
                <a href="../contents/MLing.html">
            
                    
                    Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="../contents/opting.html">
            
                <a href="../contents/opting.html">
            
                    
                    Numerical and Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="../contents/stating.html">
            
                <a href="../contents/stating.html">
            
                    
                    Traditional Statistics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="../contents/booking.html">
            
                <a href="../contents/booking.html">
            
                    
                    Books
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../contents/notes.html">
            
                <a href="../contents/notes.html">
            
                    
                    Notes
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="NeuralODE.html">
            
                <a href="NeuralODE.html">
            
                    
                    Neural ODE
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="RDN.html">
            
                <a href="RDN.html">
            
                    
                    Relational Dependency Networks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="LDA.html">
            
                <a href="LDA.html">
            
                    
                    LDA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="KSD_test.html">
            
                <a href="KSD_test.html">
            
                    
                    KSD for Goodness-of-fit Tests
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="LTK_test.html">
            
                <a href="LTK_test.html">
            
                    
                    LTK Goodness-of-Fit Test
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="GaussMixCompress.html">
            
                <a href="GaussMixCompress.html">
            
                    
                    Sample Complexity Bounds for Gaussian Mixtures
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="SGMCMC.html">
            
                <a href="SGMCMC.html">
            
                    
                    A Complete Recipe for SGMCMC
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.8" data-path="Sharpe_Minima_Works.html">
            
                <a href="Sharpe_Minima_Works.html">
            
                    
                    Sharpe Minima Can Generalize
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.9" data-path="Sharpe_Minima_Exp.html">
            
                <a href="Sharpe_Minima_Exp.html">
            
                    
                    Large Batch Training and Sharpe Minima
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.10" data-path="RMLHMC.html">
            
                <a href="RMLHMC.html">
            
                    
                    Riemann Manifold Langevin and HMC
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.11" data-path="pathSGD.html">
            
                <a href="pathSGD.html">
            
                    
                    Path-sgd: Path-normalized Optimization in Deep Neural Networks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.12" data-path="Empirical_Loss.html">
            
                <a href="Empirical_Loss.html">
            
                    
                    Empirical Analysis of DNN Loss Surfaces
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.13" data-path="Good_Semi_Bad_GAN.html">
            
                <a href="Good_Semi_Bad_GAN.html">
            
                    
                    Good Semi-supervised Learning that Requires a Bad GAN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.14" data-path="Faster_Frank_Wolfe.html">
            
                <a href="Faster_Frank_Wolfe.html">
            
                    
                    Faster Rates for Frank-Wolfe over Strongly-Convex Sets
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.15" data-path="Fin_Fan_Book.html">
            
                <a href="Fin_Fan_Book.html">
            
                    
                    The Element of Financial Econometrics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.16" data-path="Double_ML.html">
            
                <a href="Double_ML.html">
            
                    
                    Double machine learning for treatment effect
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4.17" data-path="RL_Book.html">
            
                <a href="RL_Book.html">
            
                    
                    Reinforecement Learning: An Introduction
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Reinforecement Learning: An Introduction</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon "></span><a href="#reinforcement-learning-an-introduction"><b> </b>Reinforcement Learning: An Introduction</a></li><ul><li><span class="title-icon "></span><a href="#multi-armed-bandits"><b>1. </b>Multi-armed Bandits</a></li><li><span class="title-icon "></span><a href="#finite-markov-decision-processes"><b>2. </b>Finite Markov Decision Processes</a></li><ul><li><span class="title-icon "></span><a href="#notations"><b>2.1. </b>Notations</a></li><li><span class="title-icon "></span><a href="#equations"><b>2.2. </b>Equations</a></li></ul><li><span class="title-icon "></span><a href="#dynamic-programming"><b>3. </b>Dynamic Programming</a></li><ul><li><span class="title-icon "></span><a href="#policy-evaluation"><b>3.1. </b>Policy Evaluation</a></li><li><span class="title-icon "></span><a href="#policy-improvement"><b>3.2. </b>Policy Improvement</a></li><li><span class="title-icon "></span><a href="#policy-iteration"><b>3.3. </b>Policy Iteration</a></li><li><span class="title-icon "></span><a href="#value-iteration"><b>3.4. </b>Value Iteration</a></li></ul><li><span class="title-icon "></span><a href="#monte-carlo-methods"><b>4. </b>Monte Carlo Methods</a></li><ul><li><span class="title-icon "></span><a href="#monte-carlo-prediction"><b>4.1. </b>Monte Carlo Prediction</a></li><li><span class="title-icon "></span><a href="#monte-carlo-control"><b>4.2. </b>Monte Carlo Control</a></li><li><span class="title-icon "></span><a href="#off-policy-prediction-and-control-via-importance-sampling"><b>4.3. </b>Off-policy Prediction and Control via Importance Sampling</a></li><li><span class="title-icon "></span><a href="#optimize-the-off-policy-importance-sampling"><b>4.4. </b>Optimize the Off-policy Importance Sampling</a></li></ul><li><span class="title-icon "></span><a href="#temporal-difference-learning"><b>5. </b>Temporal-Difference Learning</a></li><ul><li><span class="title-icon "></span><a href="#td0-learning"><b>5.1. </b>TD(0) Learning</a></li></ul></ul></ul></div><a href="#reinforcement-learning-an-introduction" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><h1 id="reinforcement-learning-an-introduction"><a name="reinforcement-learning-an-introduction" class="anchor-navigation-ex-anchor" href="#reinforcement-learning-an-introduction"><i class="fa fa-link" aria-hidden="true"></i></a> Reinforcement Learning: An Introduction</h1>
<h2 id="multi-armed-bandits"><a name="multi-armed-bandits" class="anchor-navigation-ex-anchor" href="#multi-armed-bandits"><i class="fa fa-link" aria-hidden="true"></i></a>1. Multi-armed Bandits</h2>
<p>Denote <script type="math/tex; ">Q(A)</script> the average reward for arm <script type="math/tex; ">A</script>, <script type="math/tex; ">N(A)</script> the number of visits to arm <script type="math/tex; ">A</script>.</p>
<p><strong><script type="math/tex; ">\epsilon</script>-greedy algorithm</strong></p>
<p><img src="pic/RL_1.png" alt=""></p>
<p><strong>Upper-Confidence-Bound algorithm</strong></p>
<p><script type="math/tex; mode=display">
A_{t} \doteq \underset{a}{\operatorname{argmax}}\left[Q_{t}(a)+c \sqrt{\frac{\ln t}{N_{t}(a)}}\right]
</script></p>
<p><strong>Gradient bandit algorithms</strong></p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\operatorname{Pr}\left\{A_{t}=a\right\} &\doteq \frac{e^{H_{t}(a)}}{\sum_{b=1}^{k} e^{H_{t}(b)}} \doteq \pi_{t}(a) \\
H_{t+1}\left(A_{t}\right) &\doteq H_{t}\left(A_{t}\right)+\alpha\left(R_{t}-\bar{R}_{t}\right)\left(1-\pi_{t}\left(A_{t}\right)\right) \\
H_{t+1}(a) &\doteq H_{t}(a)-\alpha\left(R_{t}-\bar{R}_{t}\right) \pi_{t}(a), \quad \text { for all } a \neq A_{t}
\end{aligned}
</script></p>
<h2 id="finite-markov-decision-processes"><a name="finite-markov-decision-processes" class="anchor-navigation-ex-anchor" href="#finite-markov-decision-processes"><i class="fa fa-link" aria-hidden="true"></i></a>2. Finite Markov Decision Processes</h2>
<h3 id="notations"><a name="notations" class="anchor-navigation-ex-anchor" href="#notations"><i class="fa fa-link" aria-hidden="true"></i></a>2.1. Notations</h3>
<p>Denote <script type="math/tex; ">S_t, R_t, A_t</script> as state, reward and actions at time <script type="math/tex; ">t</script>.</p>
<p><script type="math/tex; mode=display">
p\left(s^{\prime}, r \; | \; s, a\right) \doteq \operatorname{Pr}\left\{S_{t}=s^{\prime}, R_{t}=r \; | \; S_{t-1}=s, A_{t-1}=a\right\}
</script></p>
<p><script type="math/tex; mode=display">
r(s, a) \doteq \mathbb{E}\left[R_{t} \; | \; S_{t-1}=s, A_{t-1}=a\right]=\sum_{r \in \mathcal{R}} r \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime}, r \; | \; s, a\right)
</script></p>
<p>discounted return:</p>
<p><script type="math/tex; mode=display">
G_{t} \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} = R_{t+1} + \gamma G_{t+1}
</script></p>
<p>state-value function:</p>
<p><script type="math/tex; mode=display">
v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} \; | \; S_{t}=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \; | \; S_{t}=s\right], \text { for all } s \in \mathcal{S}
</script></p>
<p>action-value function:</p>
<p><script type="math/tex; mode=display">
q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \; | \; S_{t}=s, A_{t}=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \; | \; S_{t}=s, A_{t}=a\right]
</script></p>
<p>optimal state-value function:</p>
<p><script type="math/tex; mode=display">
v_{*}(s) \doteq \max _{\pi} v_{\pi}(s), \; \; \text { for all } s \in \mathcal{S}
</script></p>
<p>Optimal policies also share the same optimal action-value function:</p>
<p><script type="math/tex; mode=display">
q_{*}(s, a) \doteq \max _{\pi} q_{\pi}(s, a), \; \; \text { for all } s \in \mathcal{S} \text { and } a \in \mathcal{A}(s)
</script></p>
<h3 id="equations"><a name="equations" class="anchor-navigation-ex-anchor" href="#equations"><i class="fa fa-link" aria-hidden="true"></i></a>2.2. Equations</h3>
<p>Bellman equation for all <script type="math/tex; ">v_\pi</script>:</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
v_{\pi}(s)
&\doteq \mathbb{E}_{\pi}\left[G_{t} \; | \; S_{t}=s\right] \\
&=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} \; | \; S_{t}=s\right] \\
&=\sum_{a} \pi(a | s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \; | \; s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right], \quad \text { for all } s \in \mathcal{S}
\end{aligned}
</script></p>
<p>relation between <script type="math/tex; ">q_\pi</script> and <script type="math/tex; ">v_\pi</script>:</p>
<p><script type="math/tex; mode=display">
q_{\pi}(s, a)=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \; | \; S_{t}=s, A_{t}=a\right]
</script></p>
<p>Bellman optimality equation:</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
v_{*}(s)
&=\max _{a \in \mathcal{A}(s)} q_{\pi_{*}}(s, a) \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[G_{t} \; | \; S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_{t+1} \; | \; S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{*}\left(S_{t+1}\right) \; | \; S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \; | \; s, a\right)\left[r+\gamma v_{*}\left(s^{\prime}\right)\right]
\end{aligned}
</script></p>
<p><script type="math/tex; mode=display">
\begin{aligned}
q_{*}(s, a) &=\mathbb{E}\left[R_{t+1}+\gamma \max _{a^{\prime}} q_{*}\left(S_{t+1}, a^{\prime}\right) \; | \; S_{t}=s, A_{t}=a\right] \\
&=\sum_{s^{\prime}, r} p\left(s^{\prime}, r \; | \; s, a\right)\left[r+\gamma \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)\right] \end{aligned}
</script></p>
<h2 id="dynamic-programming"><a name="dynamic-programming" class="anchor-navigation-ex-anchor" href="#dynamic-programming"><i class="fa fa-link" aria-hidden="true"></i></a>3. Dynamic Programming</h2>
<p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). In Dynamic Programming, we know exactly what the model is, that is <script type="math/tex; ">p\left(s^{\prime}, r \; | \; s, a\right), \text { for all } s \in \mathcal{S} \text { and } a \in \mathcal{A}(s)</script>.</p>
<h3 id="policy-evaluation"><a name="policy-evaluation" class="anchor-navigation-ex-anchor" href="#policy-evaluation"><i class="fa fa-link" aria-hidden="true"></i></a>3.1. Policy Evaluation</h3>
<p><img src="pic/RL_2.png" alt=""></p>
<h3 id="policy-improvement"><a name="policy-improvement" class="anchor-navigation-ex-anchor" href="#policy-improvement"><i class="fa fa-link" aria-hidden="true"></i></a>3.2. Policy Improvement</h3>
<p><strong>Policy improvement theorem</strong>: Let <script type="math/tex; ">\pi</script> and <script type="math/tex; ">\pi^{\prime}</script> be any pair of deterministic policies such that, for all <script type="math/tex; ">s \in \mathcal{S}</script>,</p>
<p><script type="math/tex; mode=display">
q_{\pi}\left(s, \pi^{\prime}(s)\right) \geq v_{\pi}(s)
</script></p>
<p>then for all <script type="math/tex; ">s \in \mathcal{S}</script>,</p>
<p><script type="math/tex; mode=display">
v_{\pi^{\prime}}(s) \geq v_{\pi}(s)
</script></p>
<p>Therefore, the new greedy policy <script type="math/tex; ">\pi^\prime</script> is given by</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\pi^{\prime}(s)
& \doteq \underset{a}{\arg \max } q_{\pi}(s, a) \\
&=\underset{a}{\arg \max } \mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \; | \;  S_{t}=s, A_{t}=a\right] \\
&=\underset{a}{\arg \max } \sum_{s^{\prime}, r} p\left(s^{\prime}, r \; | \;  s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]
\end{aligned}
</script></p>
<p>If the new policy <script type="math/tex; ">\pi^\prime</script> is just as good as the old one <script type="math/tex; ">\pi</script>, then <script type="math/tex; ">\pi^\prime</script> is the optimal policy (satisfies Bellman optimal equation):</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
v_{\pi^{\prime}}(s)
&=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{\pi^{\prime}}\left(S_{t+1}\right) \; | \;  S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \; | \; s, a\right)\left[r+\gamma v_{\pi^{\prime}}\left(s^{\prime}\right)\right]
\end{aligned}
</script></p>
<h3 id="policy-iteration"><a name="policy-iteration" class="anchor-navigation-ex-anchor" href="#policy-iteration"><i class="fa fa-link" aria-hidden="true"></i></a>3.3. Policy Iteration</h3>
<p>Combine policy evaluation and policy improvement together, we have:</p>
<p><img src="pic/RL_3.png" alt=""></p>
<h3 id="value-iteration"><a name="value-iteration" class="anchor-navigation-ex-anchor" href="#value-iteration"><i class="fa fa-link" aria-hidden="true"></i></a>3.4. Value Iteration</h3>
<p>One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set.</p>
<p>We can consider using policy evaluation where it stops after just one sweep (one update of each state). This algorithm is called value iteration.</p>
<p><img src="pic/RL_4.png" alt=""></p>
<h2 id="monte-carlo-methods"><a name="monte-carlo-methods" class="anchor-navigation-ex-anchor" href="#monte-carlo-methods"><i class="fa fa-link" aria-hidden="true"></i></a>4. Monte Carlo Methods</h2>
<h3 id="monte-carlo-prediction"><a name="monte-carlo-prediction" class="anchor-navigation-ex-anchor" href="#monte-carlo-prediction"><i class="fa fa-link" aria-hidden="true"></i></a>4.1. Monte Carlo Prediction</h3>
<p><img src="pic/RL_5.png" alt=""></p>
<p><strong>If a model is not available, then it is particularly useful to estimate action values (the values of state&#x2013;action pairs) rather than state values. With a model (in DP case), state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state.</strong></p>
<h3 id="monte-carlo-control"><a name="monte-carlo-control" class="anchor-navigation-ex-anchor" href="#monte-carlo-control"><i class="fa fa-link" aria-hidden="true"></i></a>4.2. Monte Carlo Control</h3>
<p>The algorithm is similar with DP case. However, we need to ensure all state-action pairs have a chance to be visited.</p>
<p><img src="pic/RL_6.png" alt=""></p>
<p>We can eliminate the exploring start assumption by using a <script type="math/tex; ">\epsilon</script>-soft policy as follows:</p>
<p><img src="pic/RL_7.png" alt=""></p>
<p>It can be shown that</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
q_{\pi}\left(s, \pi^{\prime}(s)\right)
&=\sum_{a} \pi^{\prime}(a | s) q_{\pi}(s, a) \\
&=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \max _{a} q_{\pi}(s, a) \\
&\geq \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \sum_{a} \frac{\pi(a | s)-\frac{\varepsilon}{|\mathcal{A}(s)|}}{1-\varepsilon} q_{\pi}(s, a) \\
&= \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)-\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+\sum_{a} \pi(a | s) q_{\pi}(s, a) \\
&= v_{\pi}(s)
\end{aligned}
</script></p>
<p>where the greater or equal is because the sum is a weighted average with nonnegative weights summing to <script type="math/tex; ">1</script>, and as such it must be less than or equal to the largest number averaged.</p>
<p>By the policy improvement theorem <script type="math/tex; ">\pi^\prime</script> is better than <script type="math/tex; ">\pi</script> and equality can hold only when both <script type="math/tex; ">\pi^\prime</script> and <script type="math/tex; ">\pi</script> are optimal among the <script type="math/tex; ">\epsilon</script>-soft policies, that is, when they are better than or equal to all other <script type="math/tex; ">\epsilon</script>-soft policies.</p>
<h3 id="off-policy-prediction-and-control-via-importance-sampling"><a name="off-policy-prediction-and-control-via-importance-sampling" class="anchor-navigation-ex-anchor" href="#off-policy-prediction-and-control-via-importance-sampling"><i class="fa fa-link" aria-hidden="true"></i></a>4.3. Off-policy Prediction and Control via Importance Sampling</h3>
<p>A more straightforward approach is to use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy. In this case we say that learning is from data <code>off&apos;&apos; the target policy, and the overall process is termed</code>off&apos;&apos;-policy learning.</p>
<p>Consider the target policy <script type="math/tex; ">\pi</script> and the behavior policy <script type="math/tex; ">b</script>, under <script type="math/tex; ">\pi</script>, the state-action trajectory start from <script type="math/tex; ">S_t</script> has the probability,</p>
<p><script type="math/tex; mode=display">
\begin{array}{l}{\operatorname{Pr}\left\{A_{t}, S_{t+1}, A_{t+1}, \ldots, S_{T} \; | \; S_{t}, A_{t: T-1} \sim \pi\right\}} \\ {\quad=\pi\left(A_{t} \; | \; S_{t}\right) p\left(S_{t+1} \; | \; S_{t}, A_{t}\right) \pi\left(A_{t+1} \; | \; S_{t+1}\right) \cdots p\left(S_{T} \; | \; S_{T-1}, A_{T-1}\right)} \\ {\quad=\prod_{k=t}^{T-1} \pi\left(A_{k} \; | \; S_{k}\right) p\left(S_{k+1} \; | \; S_{k}, A_{k}\right)}\end{array}
</script></p>
<p>the relative probability of the trajectory under the target and behavior policies is</p>
<p><script type="math/tex; mode=display">
\rho_{t: T-1} \doteq \frac{\prod_{k=t}^{T-1} \pi\left(A_{k} \; | \; S_{k}\right) p\left(S_{k+1} \; | \; S_{k}, A_{k}\right)}{\prod_{k=t}^{T-1} b\left(A_{k} \; | \; S_{k}\right) p\left(S_{k+1} \; | \; S_{k}, A_{k}\right)}=\prod_{k=t}^{T-1} \frac{\pi\left(A_{k} \; | \; S_{k}\right)}{b\left(A_{k} \; | \; S_{k}\right)}
</script></p>
<p>Therefore,</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E} \left[G_t \; | \; S_t\right] = v_b(s), \; \; \mathbb{E}\left[\rho_{t: T-1} G_{t} | S_{t}=s\right]=v_{\pi}(s)
\end{aligned}
</script></p>
<p>To estimate <script type="math/tex; ">v_\pi(s)</script>, we simply scale the returns by the ratios and average the results:</p>
<p><script type="math/tex; mode=display">
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1} G_{t}}{|\mathcal{T}(s)|}, \; \text{ or } \; V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1} G_{t}}{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1}}
</script></p>
<p>where <script type="math/tex; ">\mathcal{T}(s)</script> would only include time steps that were first visits to s within their episodes.</p>
<p>The incremental implementation of off-policy Monte Carlo evaluation is shown below, where <script type="math/tex; ">C(S_t, A_t)</script> is the cumulative weights.</p>
<p><img src="pic/RL_8.png" alt=""></p>
<p>The corresponding off-policy MC control algorithm is as follows.</p>
<p><img src="pic/RL_9.png" alt=""></p>
<p>Notice that <script type="math/tex; ">\pi(S_t)</script> is taken as a greedy policy and only when it coincides with the path of policy $b$, the update will continue within a episode. Therefore, <script type="math/tex; ">\pi(A_t | S_t) = 1</script>.</p>
<h3 id="optimize-the-off-policy-importance-sampling"><a name="optimize-the-off-policy-importance-sampling" class="anchor-navigation-ex-anchor" href="#optimize-the-off-policy-importance-sampling"><i class="fa fa-link" aria-hidden="true"></i></a>4.4. Optimize the Off-policy Importance Sampling</h3>
<p>Define <strong>flat partial returns</strong>:</p>
<p><script type="math/tex; mode=display">
\bar{G}_{t: h} \doteq R_{t+1}+R_{t+2}+\cdots+R_{h}, \quad 0 \leq t<h \leq T
</script></p>
<p>Therefore,</p>
<p><script type="math/tex; mode=display">
\begin{aligned} G_{t}
&\doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1} R_{T} \\
&=(1-\gamma) R_{t+1} \\
&\quad +(1-\gamma) \gamma\left(R_{t+1}+R_{t+2}\right) \\
&\quad +(1-\gamma) \gamma^{2}\left(R_{t+1}+R_{t+2}+R_{t+3}\right) \\
&\quad \vdots \\
&\quad +(1-\gamma) \gamma^{T-t-2}\left(R_{t+1}+R_{t+2}+\cdots+R_{T-1}\right) \\
&\quad +\gamma^{T-t-1}\left(R_{t+1}+R_{t+2}+\cdots+R_{T}\right) \\
&= (1-\gamma) \sum_{h=t+1}^{T-1} \gamma^{h-t-1} \bar{G}_{t: h}+\gamma^{T-t-1} \bar{G}_{t: T} \end{aligned}
</script></p>
<p>And the ordinary importance-sampling estimator and weighted importance-sampling estimator can be written as,</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
V(s) &\doteq \frac{\sum_{t \in \mathcal{T}(s)}\left((1-\gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t: h-1} \bar{G}_{t: h}+\gamma^{T(t)-t-1} \rho_{t: T(t)-1} \bar{G}_{t: T(t)}\right)}{|\mathcal{T}(s)|} \\
V(s) &\doteq \frac{\sum_{t \in \mathcal{T}(s)}\left((1-\gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t: h-1} \bar{G}_{t: h}+\gamma^{T(t)-t-1} \rho_{t: T(t)-1} \bar{G}_{t: T(t)}\right)}{\sum_{t \in \mathcal{T}(s)}\left((1-\gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t: h-1}+\gamma^{T(t)-t-1} \rho_{t: T(t)-1}\right)}
\end{aligned}
</script></p>
<p>The essence of the idea is to think of discounting as determining a probability of termination or, equivalently, a degree of partial termination. Therefore, the variance of importance sampling can be reduced.</p>
<p>In the off-policy estimators, each term of the sum in the numerator is itself a sum:</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\rho_{t: T-1} G_{t}
&=\rho_{t: T-1}\left(R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-t-1} R_{T}\right) \\
&=\rho_{t: T-1} R_{t+1}+\gamma \rho_{t: T-1} R_{t+2}+\cdots+\gamma^{T-t-1} \rho_{t: T-1} R_{T}
\end{aligned}
</script></p>
<p>where</p>
<p><script type="math/tex; mode=display">
\rho_{t: T-1} R_{t+1}=\frac{\pi\left(A_{t} \; | \; S_{t}\right)}{b\left(A_{t} \; | \; S_{t}\right)} \frac{\pi\left(A_{t+1} \; | \; S_{t+1}\right)}{b\left(A_{t+1} \; | \; S_{t+1}\right)} \frac{\pi\left(A_{t+2} \; | \; S_{t+2}\right)}{b\left(A_{t+2} \; | \; S_{t+2}\right)} \cdots \frac{\pi\left(A_{T-1} \; | \; S_{T-1}\right)}{b\left(A_{T-1} \; | \; S_{T-1}\right)} R_{t+1}
</script></p>
<p>We can show that,</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[\rho_{t: T-1} R_{t+1}\right] &=\mathbb{E}\left[\rho_{t: t} R_{t+1}\right] \\
\mathbb{E}\left[\rho_{t: T-1} R_{t+k}\right] &=\mathbb{E}\left[\rho_{t: t+k-1} R_{t+k}\right]
\end{aligned}
</script></p>
<p>Thus,</p>
<p><script type="math/tex; mode=display">
\mathbb{E}\left[\rho_{t: T-1} G_{t}\right] = \mathbb{E}\left[\tilde{G}_{t}\right]
</script></p>
<p>where</p>
<p><script type="math/tex; mode=display">
\tilde{G}_{t} = \rho_{t: t} R_{t+1}+\gamma \rho_{t: t+1} R_{t+2}+\gamma^{2} \rho_{t: t+2} R_{t+3}+\cdots+\gamma^{T-t-1} \rho_{t: T-1} R_{T}
</script></p>
<p>and the ordinary/weighted average of returns can be defined accordingly. This method, known as <strong>Per-decision Importance Sampling</strong>, is useful to reduce the variance of importance sampling.</p>
<h2 id="temporal-difference-learning"><a name="temporal-difference-learning" class="anchor-navigation-ex-anchor" href="#temporal-difference-learning"><i class="fa fa-link" aria-hidden="true"></i></a>5. Temporal-Difference Learning</h2>
<h3 id="td0-learning"><a name="td0-learning" class="anchor-navigation-ex-anchor" href="#td0-learning"><i class="fa fa-link" aria-hidden="true"></i></a>5.1. TD(0) Learning</h3>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="Double_ML.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page: Double machine learning for treatment effect">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Reinforecement Learning: An Introduction","level":"1.4.17","depth":2,"previous":{"title":"Double machine learning for treatment effect","level":"1.4.16","depth":2,"path":"notes/Double_ML.md","ref":"./notes/Double_ML.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax","image-captions","github","anchors","anchor-navigation-ex","-sharing","sharing-plus@^0.0.2","github-buttons","embed-pdf","livereload"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"github":{"url":"https://github.com/ZizouHe/"},"livereload":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"fa fa-hand-o-right","level2Icon":"fa fa-hand-o-right","level3Icon":"fa fa-hand-o-right","showLevelIcon":false},"mode":"float","multipleH1":false,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":true},"github-buttons":{"buttons":[{"user":"ZizouHe","repo":"reading-notes","type":"star","size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"qq":true,"all":["facebook","google","twitter","weibo","qq","linkedin"],"douban":false,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":false,"messenger":false,"line":false,"vk":false,"pocket":false,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"anchors":{},"embed-pdf":{},"image-captions":{"caption":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","align":"right","variable_name":"_pictures"}},"theme":"default","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{"_pictures":[{"backlink":"./notes/RDN.html#fig1.4.2.1","level":"1.4.2","align":"right","list_caption":"Figure: Example (a) data graph and (b) model graph.","alt":"Example (a) data graph and (b) model graph.","nro":1,"url":"./pic/RDN-1.png","index":1,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Example (a) data graph and (b) model graph.","attributes":{},"skip":false,"key":"1.4.2.1"},{"backlink":"./notes/RDN.html#fig1.4.2.2","level":"1.4.2","align":"right","list_caption":"Figure: RDN inference algorithm.","alt":"RDN inference algorithm.","nro":2,"url":"./pic/RDN-2.png","index":2,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"RDN inference algorithm.","attributes":{},"skip":false,"key":"1.4.2.2"},{"backlink":"./notes/Faster_Frank_Wolfe.html#fig1.4.14.1","level":"1.4.14","align":"right","list_caption":"Figure: Frank-Wolfe Algorithm","alt":"Frank-Wolfe Algorithm","nro":3,"url":"./pic/Frank-Wolfe.png","index":1,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Frank-Wolfe Algorithm","attributes":{},"skip":false,"key":"1.4.14.1"},{"backlink":"./notes/Fin_Fan_Book.html#fig1.4.15.1","level":"1.4.15","align":"right","list_caption":"Figure: Efficient Frontier with risk-free asset","alt":"Efficient Frontier with risk-free asset","nro":4,"url":"./pic/Eff_Front_1.png","index":1,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Efficient Frontier with risk-free asset","attributes":{},"skip":false,"key":"1.4.15.1"},{"backlink":"./notes/Fin_Fan_Book.html#fig1.4.15.2","level":"1.4.15","align":"right","list_caption":"Figure: Efficient Frontier without risk-free asset","alt":"Efficient Frontier without risk-free asset","nro":5,"url":"./pic/Eff_Front_2.png","index":2,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Efficient Frontier without risk-free asset","attributes":{},"skip":false,"key":"1.4.15.2"}]},"title":"Reading List & Notes","gitbook":"*"},"file":{"path":"notes/RL_Book.md","mtime":"2019-10-16T20:57:52.054Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-10-16T19:27:32.635Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

