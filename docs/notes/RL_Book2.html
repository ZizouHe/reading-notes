
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Part 2: Reinforcement Learning: An Introduction (2) · Reading List & Notes</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-image-captions/image-captions.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchors/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="TRPO.html" />
    
    
    <link rel="prev" href="RL_Book1.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"ZizouHe","repo":"reading-notes","type":"star","size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../contents/already.html">
            
                <a href="../contents/already.html">
            
                    
                    Finished List
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../contents/bayes.html">
            
                <a href="../contents/bayes.html">
            
                    
                    Bayesian Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../contents/DL.html">
            
                <a href="../contents/DL.html">
            
                    
                    Deep Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../contents/fin.html">
            
                <a href="../contents/fin.html">
            
                    
                    Finance/Quant Finance
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../contents/CS.html">
            
                <a href="../contents/CS.html">
            
                    
                    General CS
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="../contents/DS.html">
            
                <a href="../contents/DS.html">
            
                    
                    General Data Science
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../contents/ML.html">
            
                <a href="../contents/ML.html">
            
                    
                    Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../contents/opt.html">
            
                <a href="../contents/opt.html">
            
                    
                    Numerical and Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="../contents/stat.html">
            
                <a href="../contents/stat.html">
            
                    
                    Traditional Statistics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="../contents/book.html">
            
                <a href="../contents/book.html">
            
                    
                    Books
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10" data-path="../contents/broker.html">
            
                <a href="../contents/broker.html">
            
                    
                    Broker Reports
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11" data-path="../contents/qfn.html">
            
                <a href="../contents/qfn.html">
            
                    
                    Quant Finance Notes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12" data-path="../contents/mln.html">
            
                <a href="../contents/mln.html">
            
                    
                    Machine Learning Notes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13" data-path="temp.html">
            
                <a href="temp.html">
            
                    
                    Temp files
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14" data-path="o3mini.html">
            
                <a href="o3mini.html">
            
                    
                    O3-mini files
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../contents/reading.html">
            
                <a href="../contents/reading.html">
            
                    
                    Reading List
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../contents/bayesing.html">
            
                <a href="../contents/bayesing.html">
            
                    
                    Bayesian Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../contents/DLing.html">
            
                <a href="../contents/DLing.html">
            
                    
                    Deep Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="../contents/MLing.html">
            
                <a href="../contents/MLing.html">
            
                    
                    Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="../contents/opting.html">
            
                <a href="../contents/opting.html">
            
                    
                    Numerical and Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="../contents/stating.html">
            
                <a href="../contents/stating.html">
            
                    
                    Traditional Statistics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="../contents/booking.html">
            
                <a href="../contents/booking.html">
            
                    
                    Books
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../contents/notes.html">
            
                <a href="../contents/notes.html">
            
                    
                    Notes
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="NeuralODE.html">
            
                <a href="NeuralODE.html">
            
                    
                    Neural ODE
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="RDN.html">
            
                <a href="RDN.html">
            
                    
                    Relational Dependency Networks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="LDA.html">
            
                <a href="LDA.html">
            
                    
                    LDA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="KSD_test.html">
            
                <a href="KSD_test.html">
            
                    
                    KSD for Goodness-of-fit Tests
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="LTK_test.html">
            
                <a href="LTK_test.html">
            
                    
                    LTK Goodness-of-Fit Test
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="GaussMixCompress.html">
            
                <a href="GaussMixCompress.html">
            
                    
                    Sample Complexity Bounds for Gaussian Mixtures
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="SGMCMC.html">
            
                <a href="SGMCMC.html">
            
                    
                    A Complete Recipe for SGMCMC
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.8" data-path="Sharpe_Minima_Works.html">
            
                <a href="Sharpe_Minima_Works.html">
            
                    
                    Sharpe Minima Can Generalize
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.9" data-path="Sharpe_Minima_Exp.html">
            
                <a href="Sharpe_Minima_Exp.html">
            
                    
                    Large Batch Training and Sharpe Minima
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.10" data-path="RMLHMC.html">
            
                <a href="RMLHMC.html">
            
                    
                    Riemann Manifold Langevin and HMC
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.11" data-path="pathSGD.html">
            
                <a href="pathSGD.html">
            
                    
                    Path-SGD: Path-normalized Optimization in Deep Neural Networks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.12" data-path="Empirical_Loss.html">
            
                <a href="Empirical_Loss.html">
            
                    
                    Empirical Analysis of DNN Loss Surfaces
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.13" data-path="Good_Semi_Bad_GAN.html">
            
                <a href="Good_Semi_Bad_GAN.html">
            
                    
                    Good Semi-supervised Learning that Requires a Bad GAN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.14" data-path="Faster_Frank_Wolfe.html">
            
                <a href="Faster_Frank_Wolfe.html">
            
                    
                    Faster Rates for Frank-Wolfe over Strongly-Convex Sets
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.15" data-path="Fin_Fan_Book.html">
            
                <a href="Fin_Fan_Book.html">
            
                    
                    The Element of Financial Econometrics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.16" data-path="Double_ML.html">
            
                <a href="Double_ML.html">
            
                    
                    Double machine learning for treatment effect
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.17" data-path="RL_Book1.html">
            
                <a href="RL_Book1.html">
            
                    
                    Part 1: Reinforcement Learning: An Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4.18" data-path="RL_Book2.html">
            
                <a href="RL_Book2.html">
            
                    
                    Part 2: Reinforcement Learning: An Introduction (2)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.19" data-path="TRPO.html">
            
                <a href="TRPO.html">
            
                    
                    Trust Region Policy Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.20" data-path="Prior_Exp_Replay.html">
            
                <a href="Prior_Exp_Replay.html">
            
                    
                    Prioritized Experience Replay
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.21" data-path="stein-galton.html">
            
                <a href="stein-galton.html">
            
                    
                    A Galtonian Perspective on Shrinkage Estimators
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.22" data-path="lasso.html">
            
                <a href="lasso.html">
            
                    
                    Regression Shrinkage and Selection via the Lasso
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.23" data-path="nonpa_inst.html">
            
                <a href="nonpa_inst.html">
            
                    
                    Nonparametric Instrumental Regression
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.24" data-path="defi_RFS.html">
            
                <a href="defi_RFS.html">
            
                    
                    Decentralized mining in centralized pools
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.25" data-path="ES_2014_majority.html">
            
                <a href="ES_2014_majority.html">
            
                    
                    Majority is not enough: Bitcoin mining is vulnerable
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.26" data-path="lazy-prices.html">
            
                <a href="lazy-prices.html">
            
                    
                    Lazy Prices
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.27" data-path="wager_athey_pnas_2021.html">
            
                <a href="wager_athey_pnas_2021.html">
            
                    
                    Confidence Intervals for Policy Evaluation in Adaptive Experiments
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.28" data-path="kelly_zhang_2021_nips.html">
            
                <a href="kelly_zhang_2021_nips.html">
            
                    
                    Statistical Inference with M-Estimators on Adaptively Collected Data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.29" data-path="realized_var.html">
            
                <a href="realized_var.html">
            
                    
                    Realized Variance and Market Microstructure Noise
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.30" data-path="PoS.html">
            
                <a href="PoS.html">
            
                    
                    Blockchain without waste: Proof-of-stake
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.31" data-path="green_2019_jfe.html">
            
                <a href="green_2019_jfe.html">
            
                    
                    Crowdsourced employer reviews and stock returns
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.32" data-path="SoK.html">
            
                <a href="SoK.html">
            
                    
                    A Survey of Attacks on Ethereum Smart Contracts
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.33" data-path="Lee_2019_jfe.html">
            
                <a href="Lee_2019_jfe.html">
            
                    
                    Technological Links and Predictable Returns
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.34" data-path="Da_2021_jfe.html">
            
                <a href="Da_2021_jfe.html">
            
                    
                    Extrapolative beliefs in the cross-section: What can we learn from the crowds?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.35" data-path="Parsons_2020_rfs.html">
            
                <a href="Parsons_2020_rfs.html">
            
                    
                    Geographic Lead-Lag Effects
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.36" data-path="Ali_2020_jfe.html">
            
                <a href="Ali_2020_jfe.html">
            
                    
                    Shared Analyst Coverage: Unifying Momentum Spillover Effects
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.37" data-path="q-factor.html">
            
                <a href="q-factor.html">
            
                    
                    Q-Factor Model
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.38" data-path="DHS-4-factors.html">
            
                <a href="DHS-4-factors.html">
            
                    
                    Short- and Long-Horizon Behavioral Factors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.39" data-path="Stambaugh-Yuan-2017.html">
            
                <a href="Stambaugh-Yuan-2017.html">
            
                    
                    Stambaugh-Yuan Four Factors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.40" data-path="cn_nlp.html">
            
                <a href="cn_nlp.html">
            
                    
                    Chinese NLP
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.41" data-path="cn_shell.html">
            
                <a href="cn_shell.html">
            
                    
                    Chinese Stock Market Shell Value
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.42" data-path="cn_ff3.html">
            
                <a href="cn_ff3.html">
            
                    
                    Size and Value
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.43" data-path="retail.html">
            
                <a href="retail.html">
            
                    
                    Tracking Retail Investor Activity
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.44" data-path="GCN.html">
            
                <a href="GCN.html">
            
                    
                    Graph Convolution and Graph Attention
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.45" data-path="phack.html">
            
                <a href="phack.html">
            
                    
                    P-hacking
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.46" data-path="general_tech_dl.html">
            
                <a href="general_tech_dl.html">
            
                    
                    General Structures and Techniques in Deep Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.47" data-path="TCN.html">
            
                <a href="TCN.html">
            
                    
                    Temporal Convolutional Network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.48" data-path="whichbeta.html">
            
                <a href="whichbeta.html">
            
                    
                    Which Beta?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.49" data-path="OOS.html">
            
                <a href="OOS.html">
            
                    
                    Out of Sample Predictability
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.50" data-path="FF3_cryoto.html">
            
                <a href="FF3_cryoto.html">
            
                    
                    Multi-factor in Cryptocurrency
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.51" data-path="quant_ml.html">
            
                <a href="quant_ml.html">
            
                    
                    Quant Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.52" data-path="behave.html">
            
                <a href="behave.html">
            
                    
                    Behavioral Finance
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.53" data-path="d-reduct.html">
            
                <a href="d-reduct.html">
            
                    
                    Nonlinear Dimension Reduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.54" data-path="moment.html">
            
                <a href="moment.html">
            
                    
                    Momentum
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.55" data-path="miscell.html">
            
                <a href="miscell.html">
            
                    
                    Lottery-like Stocks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.56" data-path="tabnet.html">
            
                <a href="tabnet.html">
            
                    
                    TabNet
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.57" data-path="kaggle-fin.html">
            
                <a href="kaggle-fin.html">
            
                    
                    Kaggle Finance Competition Solution Summary
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.58" data-path="feature_int.html">
            
                <a href="feature_int.html">
            
                    
                    Feature Interaction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.59" data-path="ESG.html">
            
                <a href="ESG.html">
            
                    
                    ESG
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.60" data-path="ivol.html">
            
                <a href="ivol.html">
            
                    
                    Idiosyncratic Volatility
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.61" data-path="volume.html">
            
                <a href="volume.html">
            
                    
                    Volume
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.62" data-path="overnight.html">
            
                <a href="overnight.html">
            
                    
                    Overnight Return Reserval
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.63" data-path="NAS.html">
            
                <a href="NAS.html">
            
                    
                    Neural Architecture Search
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.64" data-path="meta.html">
            
                <a href="meta.html">
            
                    
                    Meta Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.65" data-path="nominal_illusion.html">
            
                <a href="nominal_illusion.html">
            
                    
                    Nominal Price Ilusion
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.66" data-path="co-trade.html">
            
                <a href="co-trade.html">
            
                    
                    Co-trade Network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.67" data-path="COI_flow_decomp.html">
            
                <a href="COI_flow_decomp.html">
            
                    
                    Order Imbalance with Trade Flow Decomposition
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.68" data-path="mlts.html">
            
                <a href="mlts.html">
            
                    
                    Time Seris Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.69" data-path="PEAD.html">
            
                <a href="PEAD.html">
            
                    
                    PEAD
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.70" data-path="risk_parity.html">
            
                <a href="risk_parity.html">
            
                    
                    Risk Parity
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.71" data-path="nlp_fin.html">
            
                <a href="nlp_fin.html">
            
                    
                    NLP in Finance
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.72" data-path="self_super_tab.html">
            
                <a href="self_super_tab.html">
            
                    
                    Self-supervised Learning for Tabular Data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.73" data-path="regime.html">
            
                <a href="regime.html">
            
                    
                    Regime Modeling
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.74" data-path="fin_stat.html">
            
                <a href="fin_stat.html">
            
                    
                    Financial Statement Related
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.75" data-path="dl_fin.html">
            
                <a href="dl_fin.html">
            
                    
                    Deep Learning in Finance
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Part 2: Reinforcement Learning: An Introduction (2)</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon "></span><a href="#reinforcement-learning-an-introduction-2"><b> </b>Reinforcement Learning: An Introduction (2)</a></li><ul><li><span class="title-icon "></span><a href="#on-policy-prediction-with-approximation"><b>1. </b>On-policy Prediction with Approximation</a></li><ul><li><span class="title-icon "></span><a href="#stochastic-gradient-and-semi-gradient-methods-for-on-policy-approximation"><b>1.1. </b>Stochastic-gradient and Semi-gradient Methods for On-policy Approximation</a></li><li><span class="title-icon "></span><a href="#linear-methods"><b>1.2. </b>Linear Methods</a></li><li><span class="title-icon "></span><a href="#feature-construction-for-linear-methods"><b>1.3. </b>Feature Construction for Linear Methods</a></li><li><span class="title-icon "></span><a href="#nonlinear-function-approximation-artificial-neural-networks"><b>1.4. </b>Nonlinear Function Approximation: Artificial Neural Networks</a></li><li><span class="title-icon "></span><a href="#least-squares-td"><b>1.5. </b>Least-Squares TD</a></li></ul><li><span class="title-icon "></span><a href="#on-policy-control-with-approximation"><b>2. </b>On-policy Control with Approximation</a></li><ul><li><span class="title-icon "></span><a href="#episodic-semi-gradient-control"><b>2.1. </b>Episodic Semi-gradient Control</a></li><li><span class="title-icon "></span><a href="#average-reward-a-new-problem-setting-for-continuing-tasks"><b>2.2. </b>Average Reward: A New Problem Setting for Continuing Tasks</a></li><li><span class="title-icon "></span><a href="#differential-semi-gradient-n-step-sarsa"><b>2.3. </b>Differential Semi-gradient n-step Sarsa</a></li></ul><li><span class="title-icon "></span><a href="#off-policy-methods-with-approximation"><b>3. </b>Off-policy Methods with Approximation</a></li><ul><li><span class="title-icon "></span><a href="#semi-gradient-methods"><b>3.1. </b>Semi-gradient Methods</a></li><li><span class="title-icon "></span><a href="#examples-of-off-policy-divergence"><b>3.2. </b>Examples of Off-policy Divergence</a></li><li><span class="title-icon "></span><a href="#linear-value-function-geometry"><b>3.3. </b>Linear Value-function Geometry</a></li><li><span class="title-icon "></span><a href="#gradient-descent-in-the-bellman-error"><b>3.4. </b>Gradient Descent in the Bellman Error</a></li><li><span class="title-icon "></span><a href="#the-bellman-error-is-not-learnable"><b>3.5. </b>The Bellman Error is Not Learnable</a></li><li><span class="title-icon "></span><a href="#gradient-td-methods"><b>3.6. </b>Gradient-TD Methods</a></li></ul><li><span class="title-icon "></span><a href="#eligibility-traces"><b>4. </b>Eligibility Traces</a></li><ul><li><span class="title-icon "></span><a href="#td-lambda"><b>4.1. </b>TD-lambda</a></li><li><span class="title-icon "></span><a href="#online-td-lambda"><b>4.2. </b>Online TD-lambda</a></li><li><span class="title-icon "></span><a href="#off-policy-traces-with-control-variates"><b>4.3. </b>Off-policy Traces with Control Variates</a></li><li><span class="title-icon "></span><a href="#eligibility-traces-q-learning"><b>4.4. </b>Eligibility Traces Q-learning</a></li><li><span class="title-icon "></span><a href="#stable-off-policy-methods-with-traces"><b>4.5. </b>Stable Off-policy Methods with Traces</a></li></ul><li><span class="title-icon "></span><a href="#policy-gradient-methods"><b>5. </b>Policy Gradient Methods</a></li><ul><li><span class="title-icon "></span><a href="#policy-approximation-and-the-policy-gradient-theorem"><b>5.1. </b>Policy Approximation and the Policy Gradient Theorem</a></li><li><span class="title-icon "></span><a href="#reinforce-monte-carlo-policy-gradient"><b>5.2. </b>REINFORCE: Monte Carlo Policy Gradient</a></li><li><span class="title-icon "></span><a href="#reinforce-with-baseline"><b>5.3. </b>REINFORCE with Baseline</a></li><li><span class="title-icon "></span><a href="#actor&#x2013;critic-methods"><b>5.4. </b>Actor&#x2013;Critic Methods</a></li><li><span class="title-icon "></span><a href="#policy-gradient-for-continuing-problems"><b>5.5. </b>Policy Gradient for Continuing Problems</a></li></ul></ul></ul></div><a href="#reinforcement-learning-an-introduction-2" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><h1 id="reinforcement-learning-an-introduction-2"><a name="reinforcement-learning-an-introduction-2" class="anchor-navigation-ex-anchor" href="#reinforcement-learning-an-introduction-2"><i class="fa fa-link" aria-hidden="true"></i></a> Reinforcement Learning: An Introduction (2)</h1>
<h2 id="on-policy-prediction-with-approximation"><a name="on-policy-prediction-with-approximation" class="anchor-navigation-ex-anchor" href="#on-policy-prediction-with-approximation"><i class="fa fa-link" aria-hidden="true"></i></a>1. On-policy Prediction with Approximation</h2>
<h3 id="stochastic-gradient-and-semi-gradient-methods-for-on-policy-approximation"><a name="stochastic-gradient-and-semi-gradient-methods-for-on-policy-approximation" class="anchor-navigation-ex-anchor" href="#stochastic-gradient-and-semi-gradient-methods-for-on-policy-approximation"><i class="fa fa-link" aria-hidden="true"></i></a>1.1. Stochastic-gradient and Semi-gradient Methods for On-policy Approximation</h3>
<p>Given a state distribution <script type="math/tex; ">\mu(s) \geq 0, \; \sum \mu(s) = 1</script>, we obtain a natural objective function, the <em>Mean Squared Value Error</em>, denoted <script type="math/tex; ">\overline{\mathrm{VE}}</script>:</p>
<p><script type="math/tex; mode=display">
\overline{\mathrm{VE}}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s)\left[v_{\pi}(s)-\hat{v}(s, \mathbf{w})\right]^{2}
</script></p>
<p>Using stochastic gradient-descent (SGD),</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+1}
&\doteq \mathbf{w}_{t}-\frac{1}{2} \alpha \nabla\left[v_{\pi}\left(S_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right]^{2} \\
&=\mathbf{w}_{t}+\alpha\left[v_{\pi}\left(S_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
\end{aligned}
</script></p>
<p>Therefore, the SGD version for Monte Carlo state-value prediction algorithm is as follows.</p>
<p><img src="pic/RL_book/RL_20.png" alt=""></p>
<p>If we use bootstrapping estimate of <script type="math/tex; ">v_\pi(S_t)</script> such as n-step TD, then this estimate relies on the current weights <script type="math/tex; ">\mathbf{w}_{t}</script> and the expression above is biased and will not produce true gradient descent. We call this kind of methods as semi-gradient methods.</p>
<p><img src="pic/RL_book/RL_21.png" alt=""></p>
<h3 id="linear-methods"><a name="linear-methods" class="anchor-navigation-ex-anchor" href="#linear-methods"><i class="fa fa-link" aria-hidden="true"></i></a>1.2. Linear Methods</h3>
<p>Denote <script type="math/tex; ">\boldsymbol{x}(s)</script> as the feature vector for state <script type="math/tex; ">s \in \mathcal{S}</script>. Linear methods approximate state-value function by the inner product between <script type="math/tex; ">\mathbf{w}</script> and <script type="math/tex; ">\mathbf{x}(s)</script>:</p>
<p><script type="math/tex; mode=display">
\hat{v}(s, \mathbf{w}) \doteq \mathbf{w}^{\top} \mathbf{x}(s) \doteq \sum_{i=1}^{d} w_{i} x_{i}(s)
</script></p>
<p>Thus, in the linear case the general SGD update reduces to a particularly simple form:</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha\left[U_{t}-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right] \mathbf{x}\left(S_{t}\right)
</script></p>
<p>The semi-gradient TD(0) algorithm presented in the previous section also converges under linear function approximation, but this does not follow from general results on SGD; a separate theorem is necessary. The weight vector converged to is also not the global optimum, but rather a point near the local optimum.</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+1}
&\doteq \mathbf{w}_{t}+\alpha\left(R_{t+1}+\gamma \mathbf{w}_{t}^{\top} \mathbf{x}_{t+1}-\mathbf{w}_{t}^{\top} \mathbf{x}_{t}\right) \mathbf{x}_{t} \\
&=\mathbf{w}_{t}+\alpha\left(R_{t+1} \mathbf{x}_{t}-\mathbf{x}_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right)^{\top} \mathbf{w}_{t}\right) \end{aligned}
</script></p>
<p>Once the system has reached steady state, for any given <script type="math/tex; ">\mathbf{w}_{t}</script>, the expected next weight vector can be written</p>
<p><script type="math/tex; mode=display">
\mathbb{E}\left[\mathbf{w}_{t+1} \; | \; \mathbf{w}_{t}\right]=\mathbf{w}_{t}+\alpha\left(\mathbf{b}-\mathbf{A} \mathbf{w}_{t}\right)
</script></p>
<p>where</p>
<p><script type="math/tex; mode=display">
\mathbf{b} \doteq \mathbb{E}\left[R_{t+1} \mathbf{x}_{t}\right] \in \mathbb{R}^{d} \quad \text { and } \quad \mathbf{A} \doteq \mathbb{E}\left[\mathbf{x}_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right)^{\top}\right] \in \mathbb{R}^{d} \times \mathbb{R}^{d}
</script></p>
<p>if the system converges, it must converge to the weight vector <script type="math/tex; ">\mathbf{w}_{\mathrm{TD}} \doteq \mathbf{A}^{-1} \mathbf{b}</script> which is called the TD fixed point. In fact linear semi-gradient TD(0) converges to this point.</p>
<p>At the TD fixed point, it has also been proven (in the continuing case) that the <script type="math/tex; ">\overline{\mathrm{VE}}</script> is within a bounded expansion of the lowest possible error:</p>
<p><script type="math/tex; mode=display">
\overline{\mathrm{VE}}\left(\mathbf{w}_{\mathrm{TD}}\right) \leq \frac{1}{1-\gamma} \min _{\mathbf{w}} \overline{\mathrm{VE}}(\mathbf{w})
</script></p>
<p>The semi-gradient <script type="math/tex; ">n</script>-step TD algorithm is the natural extension of the tabular <script type="math/tex; ">n</script>-step TD algorithm presented to semi-gradient function approximation.</p>
<p><img src="pic/RL_book/RL_22.png" alt=""></p>
<p>where the <script type="math/tex; ">n</script>-step return is generalized as</p>
<p><script type="math/tex; mode=display">
G_{t: t+n} \doteq R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1} R_{t+n}+\gamma^{n} \hat{v}\left(S_{t+n}, \mathbf{w}_{t+n-1}\right), \quad 0 \leq t \leq T-n
</script></p>
<h3 id="feature-construction-for-linear-methods"><a name="feature-construction-for-linear-methods" class="anchor-navigation-ex-anchor" href="#feature-construction-for-linear-methods"><i class="fa fa-link" aria-hidden="true"></i></a>1.3. Feature Construction for Linear Methods</h3>
<p><strong>Polynomials</strong></p>
<p><strong>Fourier Basis</strong></p>
<p><strong>Radial Basis Functions</strong></p>
<p><script type="math/tex; mode=display">
x_{i}(s) \doteq \exp \left(-\frac{\left\|s-c_{i}\right\|^{2}}{2 \sigma_{i}^{2}}\right)
</script></p>
<p><strong>Kernel-based Function</strong></p>
<p><strong>Coarse Coding</strong>&#xFF1A;</p>
<p>Consider a task in which the natural representation of the state set is a continuous two- dimensional space. One kind of representation for this case is made up of features corresponding to circles in state space, as shown to the right. If the state is inside a circle, then the corresponding feature has the value 1 and is said to be present; otherwise the feature is 0 and is said to be absent. This kind of <script type="math/tex; ">1–0</script>-valued feature is called a binary feature. Given a state, which binary features are present indicate within which circles the state lies, and thus coarsely code for its location. Representing a state with features that overlap in this way (although they need not be circles or binary) is known as coarse coding.</p>
<p><img src="pic/RL_book/RL_23.png" alt=""></p>
<p><strong>Tile Coding</strong></p>
<p>Tile coding is a form of coarse coding for multi-dimensional continuous spaces that is
flexible and computationally efficient. It may be the most practical feature representation
for modern sequential digital computers. In tile coding the receptive fields of the features are grouped into partitions of the state space. Each such partition is called a tiling, and each element of the partition is called a tile.</p>
<p><img src="pic/RL_book/RL_24.png" alt=""></p>
<h3 id="nonlinear-function-approximation-artificial-neural-networks"><a name="nonlinear-function-approximation-artificial-neural-networks" class="anchor-navigation-ex-anchor" href="#nonlinear-function-approximation-artificial-neural-networks"><i class="fa fa-link" aria-hidden="true"></i></a>1.4. Nonlinear Function Approximation: Artificial Neural Networks</h3>
<h3 id="least-squares-td"><a name="least-squares-td" class="anchor-navigation-ex-anchor" href="#least-squares-td"><i class="fa fa-link" aria-hidden="true"></i></a>1.5. Least-Squares TD</h3>
<p>Recall TD fixed point</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{\mathrm{TD}} \doteq \mathbf{A}^{-1} \mathbf{b}
</script></p>
<p>where</p>
<p><script type="math/tex; mode=display">
\mathbf{b} \doteq \mathbb{E}\left[R_{t+1} \mathbf{x}_{t}\right] \in \mathbb{R}^{d} \quad \text { and } \quad \mathbf{A} \doteq \mathbb{E}\left[\mathbf{x}_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right)^{\top}\right] \in \mathbb{R}^{d} \times \mathbb{R}^{d}
</script></p>
<p>The Least-Squares TD algorithm, commonly known as LSTD, forms the natural estimates</p>
<p><script type="math/tex; mode=display">
\widehat{\mathbf{A}}_{t} \doteq \sum_{k=0}^{t-1} \mathbf{x}_{k}\left(\mathbf{x}_{k}-\gamma \mathbf{x}_{k+1}\right)^{\top}+\varepsilon \mathbf{I} \quad \text { and } \quad \widehat{\mathbf{b}}_{t} \doteq \sum_{k=0}^{t-1} R_{k+1} \mathbf{x}_{k}
</script></p>
<p>where LSTD uses these estimates to estimate the TD fixed point as</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t} \doteq \widehat{\mathbf{A}}_{t}^{-1} \widehat{\mathbf{b}}_{t}
</script></p>
<p>In LSTD, the inverse of <script type="math/tex; ">\widehat{\mathbf{A}}_{t}</script>, and the computational complexity of a general inverse computation is <script type="math/tex; ">O\left(d^{3}\right)</script>. Fortunately, an inverse of a matrix of our special form - a sum of outer products - can also be updated incrementally with only <script type="math/tex; ">O\left(d^{2}\right)</script> computations, as</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\widehat{\mathbf{A}}_{t}^{-1}
&=\left(\widehat{\mathbf{A}}_{t-1}+\mathbf{x}_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right)^{\top}\right)^{-1} \\
&=\widehat{\mathbf{A}}_{t-1}^{-1}-\frac{\widehat{\mathbf{A}}_{t-1}^{-1} \mathbf{x}_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right)^{\top} \widehat{\mathbf{A}}_{t-1}^{-1}}{1+\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right)^{\top} \widehat{\mathbf{A}}_{t-1}^{-1} \mathbf{x}_{t}} \end{aligned}
</script></p>
<p>for <script type="math/tex; ">t>0</script>, with <script type="math/tex; ">\widehat{\mathbf{A}}_{0} \doteq \varepsilon \mathbf{I}</script>. Although the identity known as the <strong><em>Sherman-Morrison formula</em></strong>, is superficially complicated, it involves only vector-matrix and vector-vector multiplications and thus is only <script type="math/tex; ">O\left(d^{2}\right)</script>. Thus we can store the inverse matrix <script type="math/tex; ">\widehat{\mathbf{A}}_{t}^{-1}</script>
maintain it and use it all with only <script type="math/tex; ">O\left(d^{2}\right)</script> memory and per-step computation. The complete algorithm is given below.</p>
<p><img src="pic/RL_book/RL_25.png" alt=""></p>
<h2 id="on-policy-control-with-approximation"><a name="on-policy-control-with-approximation" class="anchor-navigation-ex-anchor" href="#on-policy-control-with-approximation"><i class="fa fa-link" aria-hidden="true"></i></a>2. On-policy Control with Approximation</h2>
<h3 id="episodic-semi-gradient-control"><a name="episodic-semi-gradient-control" class="anchor-navigation-ex-anchor" href="#episodic-semi-gradient-control"><i class="fa fa-link" aria-hidden="true"></i></a>2.1. Episodic Semi-gradient Control</h3>
<p>The update for the one-step Sarsa method is</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha\left[R_{t+1}+\gamma \hat{q}\left(S_{t+1}, A_{t+1}, \mathbf{w}_{t}\right)-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right)
</script></p>
<p><img src="pic/RL_book/RL_26.png" alt=""></p>
<p>We can obtain an <script type="math/tex; ">n</script>-step version of episodic semi-gradient Sarsa by using an <script type="math/tex; ">n</script>-step return as the update target in the semi-gradient one-step Sarsa update equation</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1}+\alpha\left[G_{t: t+n}-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t+n-1}\right)\right] \nabla \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t+n-1}\right), \quad 0 \leq t<T
</script></p>
<p>where</p>
<p><script type="math/tex; mode=display">
G_{t: t+n} \doteq R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1} R_{t+n}+\gamma^{n} \hat{q}\left(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}\right), \quad t+n<T
</script></p>
<p><img src="pic/RL_book/RL_27.png" alt=""></p>
<h3 id="average-reward-a-new-problem-setting-for-continuing-tasks"><a name="average-reward-a-new-problem-setting-for-continuing-tasks" class="anchor-navigation-ex-anchor" href="#average-reward-a-new-problem-setting-for-continuing-tasks"><i class="fa fa-link" aria-hidden="true"></i></a>2.2. Average Reward: A New Problem Setting for Continuing Tasks</h3>
<p>In the average-reward setting, the quality of a policy <script type="math/tex; ">\pi</script> is defined as the average rate of
reward, or simply average reward, while following that policy, which we denote as <script type="math/tex; ">r(\pi)</script>:</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
r(\pi)
& \doteq \lim _{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}\left[R_{t}  \; | \; S_{0}, A_{0: t-1} \sim \pi\right] \\
&=\lim _{t \rightarrow \infty} \mathbb{E}\left[R_{t} \; | \; S_{0}, A_{0: t-1} \sim \pi\right] \\
&=\sum_{s} \mu_{\pi}(s) \sum_{a} \pi(a \; | \; s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \; | \; s, a\right) r \end{aligned}
</script></p>
<p>where the expectations are conditioned on the initial state, <script type="math/tex; ">S_{0}</script>, and on the subsequent
actions, <script type="math/tex; ">A_{0}, A_{1}, \ldots, A_{t-1}</script>, being taken according to <script type="math/tex; ">\pi</script>. <script type="math/tex; ">\mu_{\pi}</script> is the steady-state distribution, <script type="math/tex; ">\mu_{\pi}(s) \doteq \lim _{t \rightarrow \infty} \operatorname{Pr}\left\{S_{t}=s \; | \; A_{0: t-1} \sim \pi\right\}</script>, which is assumed to exist for any <script type="math/tex; ">\pi</script> and to be independent of <script type="math/tex; ">S_{0}</script>. This assumption about the MDP is known as ergodicity.</p>
<p>Note that the steady state distribution is the special distribution under which, if you select actions according to <script type="math/tex; ">\pi</script>, you remain in the same distribution. That is, for which</p>
<p><script type="math/tex; mode=display">
\sum_{s} \mu_{\pi}(s) \sum_{a} \pi(a \; | \; s) p\left(s^{\prime} \; | \; s, a\right)=\mu_{\pi}\left(s^{\prime}\right)
</script></p>
<p>In the average-reward setting, returns are defined in terms of differences between rewards and the average reward:</p>
<p><script type="math/tex; mode=display">
G_{t} \doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\cdots
</script></p>
<p>We simply remove all <script type="math/tex; ">\gamma</script>s and replace all rewards by the difference between the reward and the true average reward:</p>
<p><script type="math/tex; mode=display">
\begin{array}{l}{v_{\pi}(s)=\sum_{a} \pi(a | s) \sum_{r, s^{\prime}} p\left(s^{\prime}, r | s, a\right)\left[r-r(\pi)+v_{\pi}\left(s^{\prime}\right)\right]} \\ {q_{\pi}(s, a)=\sum_{r, s^{\prime}} p\left(s^{\prime}, r | s, a\right)\left[r-r(\pi)+\sum_{a^{\prime}} \pi\left(a^{\prime} | s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)\right]} \\ {v_{*}(s)=\max _{a} \sum_{r, s^{\prime}} p\left(s^{\prime}, r | s, a\right)\left[r-\max _{\pi} r(\pi)+v_{*}\left(s^{\prime}\right)\right]} \\ {q_{*}(s, a)=\sum_{r, s^{\prime}} p\left(s^{\prime}, r | s, a\right)\left[r-\max _{\pi} r(\pi)+\max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)\right]}\end{array}
</script></p>
<p><img src="pic/RL_book/RL_28.png" alt=""></p>
<h3 id="differential-semi-gradient-n-step-sarsa"><a name="differential-semi-gradient-n-step-sarsa" class="anchor-navigation-ex-anchor" href="#differential-semi-gradient-n-step-sarsa"><i class="fa fa-link" aria-hidden="true"></i></a>2.3. Differential Semi-gradient n-step Sarsa</h3>
<p>We begin by generalizing the n-step return to its differential form, with function approximation:</p>
<p><script type="math/tex; mode=display">
G_{t: t+n} \doteq R_{t+1}-\bar{R}_{t+1}+R_{t+2}-\bar{R}_{t+2}+\cdots+R_{t+n}-\bar{R}_{t+n}+\hat{q}\left(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}\right)
</script></p>
<p>where <script type="math/tex; ">\bar{R}</script> is an estimate of <script type="math/tex; ">r(\pi), n \geq 1</script>. Pseudocode for the complete algorithm is given in the box.</p>
<p><img src="pic/RL_book/RL_29.png" alt=""></p>
<p>where</p>
<p><script type="math/tex; mode=display">
\delta_{t} \doteq G_{t: t+n}-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}\right)
</script></p>
<h2 id="off-policy-methods-with-approximation"><a name="off-policy-methods-with-approximation" class="anchor-navigation-ex-anchor" href="#off-policy-methods-with-approximation"><i class="fa fa-link" aria-hidden="true"></i></a>3. Off-policy Methods with Approximation</h2>
<h3 id="semi-gradient-methods"><a name="semi-gradient-methods" class="anchor-navigation-ex-anchor" href="#semi-gradient-methods"><i class="fa fa-link" aria-hidden="true"></i></a>3.1. Semi-gradient Methods</h3>
<p>The one-step, state-value algorithm is semi-gradient off-policy TD(0)</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \rho_{t} \delta_{t} \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
</script></p>
<p>where <script type="math/tex; ">\delta_{t}</script> is defined appropriately depending on whether the problem is episodic and
discounted, or continuing and undiscounted using average reward:</p>
<p><script type="math/tex; mode=display">
\begin{array}{l}{\delta_{t} \doteq R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right), \text { or }} \\ {\delta_{t} \doteq R_{t+1}-\bar{R}_{t}+\hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)}\end{array}
</script></p>
<p>For action values, the one-step algorithm is semi-gradient Expected Sarsa:</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1}+\alpha \rho_{t+1} \cdots \rho_{t+n-1}\left[G_{t: t+n}-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t+n-1}\right)\right] \nabla \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t+n-1}\right)
</script></p>
<p>where</p>
<p><script type="math/tex; mode=display">
\begin{aligned} \delta_{t} & \doteq R_{t+1}+\gamma \sum_{a} \pi\left(a | S_{t+1}\right) \hat{q}\left(S_{t+1}, a, \mathbf{w}_{t}\right)-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right), & \text { or } & \text { (episodic) } \\ \delta_{t} & \doteq R_{t+1}-\bar{R}_{t}+\sum \pi\left(a | S_{t+1}\right) \hat{q}\left(S_{t+1}, a, \mathbf{w}_{t}\right)-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right) & & \text { (continuing) } \end{aligned}
</script></p>
<p>You can find the multi-step generalizations of these algorithms in a similar manner.</p>
<h3 id="examples-of-off-policy-divergence"><a name="examples-of-off-policy-divergence" class="anchor-navigation-ex-anchor" href="#examples-of-off-policy-divergence"><i class="fa fa-link" aria-hidden="true"></i></a>3.2. Examples of Off-policy Divergence</h3>
<p>Consider estimating the state-value under the linear parameterization indicated by
the expression shown in each state circle. For example, the estimated value of the
leftmost state is <script type="math/tex; ">2 w_{1}+w_{8}</script>, where the subscript corresponds to the component of the overall weight vector <script type="math/tex; ">\mathbf{w} \in \mathbb{R}^{8}</script>; this corresponds to a feature vector for the first state being <script type="math/tex; ">\mathbf{x}(1)=(2,0,0,0,0,0,0,1)^{\top}</script>. The reward is zero on all transitions, so the true value function is <script type="math/tex; ">v_{\pi}(s)=0</script>, for all <script type="math/tex; ">s</script>, which can be exactly approximated if <script type="math/tex; ">\mathbf{w}=\mathbf{0}</script>. In fact, there are many solutions, as there are more components to the weight vector ( 8) than there are nonterminal states (7). Moreover, the set of feature vectors, <script type="math/tex; ">\{\mathbf{x}(s): s \in \mathcal{S}\}</script>, is a linearly independent set. In all these ways this task seems a favorable case for linear function approximation.</p>
<p><img src="pic/RL_book/RL_30.png" alt=""></p>
<p>If we apply semi-gradient TD(0) to this problem, then the weights diverge to infinity. The instability occurs for any positive step size, no matter how small. In fact, it even occurs if an expected update is done as in dynamic programming (DP).</p>
<p>If we alter just the distribution of DP updates in Baird&#x2019;s counterexample, from the uniform distribution to the on-policy distribution (which generally requires asynchronous updating), then convergence is guaranteed. The example shows that even the simplest combination of bootstrapping and function approximation can be unstable if the updates are not done according to the on-policy distribution.</p>
<p>There are also counterexamples similar to Baird&#x2019;s showing divergence for Q-learning.</p>
<p>Our discussion so far can be summarized by saying that the danger of instability and divergence arises whenever we combine all of the following three elements, making up what we call <strong><em>the deadly triad</em></strong>:</p>
<ul>
<li><p><strong>Function approximation</strong> A powerful, scalable way of generalizing from a state space much larger than the memory and computational resources;</p>
</li>
<li><p><strong>Bootstrapping</strong> Update targets that include existing estimates (as in dynamic programming or TD methods) rather than relying exclusively on actual rewards and complete returns (as in MC methods).</p>
</li>
<li><p><strong>Off-policy training</strong> Training on a distribution of transitions other than that produced by the target policy. Sweeping through the state space and updating all states uniformly, as in dynamic programming, does not respect the target policy and is an example of off-policy training.</p>
</li>
</ul>
<h3 id="linear-value-function-geometry"><a name="linear-value-function-geometry" class="anchor-navigation-ex-anchor" href="#linear-value-function-geometry"><i class="fa fa-link" aria-hidden="true"></i></a>3.3. Linear Value-function Geometry</h3>
<p>We can define the distance between value functions using the norm</p>
<p><script type="math/tex; mode=display">
\|v\|_{\mu}^{2} \doteq \sum_{s \in \mathcal{S}} \mu(s) v(s)^{2}
</script></p>
<p>Notice that <script type="math/tex; ">\overline{\mathrm{VE}}</script> we use for semi-gradient method is,</p>
<p><script type="math/tex; mode=display">
\overline{\mathrm{VE}}(\mathbf{w}) \doteq \sum_{s \in S} \mu(s)\left[v_{\pi}(s)-\hat{v}(s, \mathbf{w})\right]^{2} = \left\|v_{\mathbf{w}}-v_{\pi}\right\|_{\mu}^{2}
</script></p>
<p>We define a projection operator <script type="math/tex; ">\Pi</script> that takes an arbitrary value function to the representable function that is closest in our norm:</p>
<p><script type="math/tex; mode=display">
\Pi v \doteq v_{\mathrm{w}} \quad \text { where } \quad \mathrm{w}=\underset{\mathrm{w} \in \mathbb{R}^{d}}{\arg \min }\left\|v-v_{\mathrm{w}}\right\|_{\mu}^{2}
</script></p>
<p>For a linear function approximator, the projection operation is linear, which implies that it can be represented as an <script type="math/tex; ">|\mathcal{S}| \times |\mathcal{S}|</script> matrix:</p>
<p><script type="math/tex; mode=display">
\Pi \doteq \mathbf{X}\left(\mathbf{X}^{\top} \mathbf{D} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{D}
</script></p>
<p>where <script type="math/tex; ">\mathbf{D}</script> is the <script type="math/tex; ">|\mathcal{S}| \times |\mathcal{S}|</script> matrix with <script type="math/tex; ">\mu(s)</script> the diagonal element, and <script type="math/tex; ">\mathbf{X}</script> denotes the <script type="math/tex; ">|\mathcal{S}| \times d</script> matrix whose rows are the feature vectors <script type="math/tex; ">\mathbf{x}(s)^{\top},</script> one for each state <script type="math/tex; ">s</script>.</p>
<p>Recall the Bellman equation for value function, we define Bellman error at state <script type="math/tex; ">s</script>,</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\bar{\delta}_{\mathbf{w}}(s)
& \doteq\left(\sum_{a} \pi(a | s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \; | \; s, a\right)\left[r+\gamma v_{\mathbf{w}}\left(s^{\prime}\right)\right]\right)-v_{\mathbf{w}}(s) \\
&=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\mathbf{w}}\left(S_{t+1}\right)-v_{\mathbf{w}}\left(S_{t}\right) \; | \; S_{t}=s, A_{t} \sim \pi\right]
\end{aligned}
</script></p>
<p>Therefore, the Mean Squared Bellman Error is,</p>
<p><script type="math/tex; mode=display">
\overline{\mathrm{BE}}(\mathbf{w})=\left\|\bar{\delta}_{\mathbf{w}}\right\|_{\mu}^{2}
</script></p>
<p>The Bellman error vector is a result of applying the Bellman operator <script type="math/tex; ">B_{\pi}: \mathbb{R}^{|\mathcal{S}|} \rightarrow \mathbb{R}^{|\mathcal{S}|}</script> to the approximate value function. The Bellman operator is,</p>
<p><script type="math/tex; mode=display">
\left(B_{\pi} v\right)(s) \doteq \sum_{a} \pi(a \; | \; s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \; | \; s, a\right)\left[r+\gamma v\left(s^{\prime}\right)\right]
</script></p>
<p>The Bellman error vector for <script type="math/tex; ">v</script> can be written <script type="math/tex; ">\bar{\delta}_{\mathbf{w}}=B_{\pi} v_{\mathbf{w}}-v_{\mathbf{w}}</script>.</p>
<p>If the Bellman operator is applied to a value function in the representable subspace, then, in general, it will produce a new value function that is outside the subspace. In dynamic programming (without function approximation), this operator is applied repeatedly to the points outside the representable space. Eventually that process converges to the true value function <script type="math/tex; ">v_\pi</script>, the only fixed point for the Bellman operator.</p>
<p>With function approximation, however, the intermediate value functions lying outside the subspace cannot be represented. In this case we are interested in the projection of the Bellman error vector back into the representable space. This is the projected Bellman error vector <script type="math/tex; ">\Pi \bar{\delta}_{v_{\mathrm{w}}}</script>. Thus, we define the Mean Square Projected Bellman Error,</p>
<p><script type="math/tex; mode=display">
\overline{\operatorname{PBE}}(\mathbf{w})=\left\|\Pi \bar{\delta}_{\mathbf{w}}\right\|_{\mu}^{2}
</script></p>
<h3 id="gradient-descent-in-the-bellman-error"><a name="gradient-descent-in-the-bellman-error" class="anchor-navigation-ex-anchor" href="#gradient-descent-in-the-bellman-error"><i class="fa fa-link" aria-hidden="true"></i></a>3.4. Gradient Descent in the Bellman Error</h3>
<p>The one-step TD error with discounting is</p>
<p><script type="math/tex; mode=display">
\delta_{t}=R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
</script></p>
<p>Therefore,</p>
<p><script type="math/tex; mode=display">
\begin{aligned} \mathbf{w}_{t+1} &=\mathbf{w}_{t}-\frac{1}{2} \alpha \nabla\left(\mathbb{E}_{\pi}\left[\delta_{t}\right]^{2}\right) \\ &=\mathbf{w}_{t}-\frac{1}{2} \alpha \nabla\left(\mathbb{E}_{b}\left[\rho_{t} \delta_{t}\right]^{2}\right) \\ &\left.=\mathbf{w}_{t}-\alpha \mathbb{E}_{b}\left[\rho_{t} \delta_{t}\right] \nabla \mathbb{E}_{b}\left[\rho_{t+1}, \mathbf{w}\right)-\hat{v}\left(S_{t}, \mathbf{w}\right)\right] \mathbb{E}_{b}\left[\rho_{t} \nabla \delta_{t}\right] \\ &=\mathbf{w}_{t}+\alpha\left[\mathbb{E}_{b}\left[\rho_{t}\left(R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}\right)\right)\right]-\hat{v}\left(S_{t}, \mathbf{w}\right)\right]\left[\nabla \hat{v}\left(S_{t}, \mathbf{w}\right)-\gamma \mathbb{E}_{b}\left[\rho_{t} \nabla \hat{v}\left(S_{t+1}, \mathbf{w}\right)\right]\right] \end{aligned}
</script></p>
<p>This update and various ways of sampling it are referred to as the residual-gradient algorithm. If you simply used the sample values in all the expectations, the equation above involves the next state, <script type="math/tex; ">S_{t+1}</script>, appearing in two expectations that are multiplied together. To get an unbiased sample of the product, two independent samples of the next state are required, but during normal interaction with an external environment only one is obtained. One expectation or the other can be sampled, but not both.</p>
<h3 id="the-bellman-error-is-not-learnable"><a name="the-bellman-error-is-not-learnable" class="anchor-navigation-ex-anchor" href="#the-bellman-error-is-not-learnable"><i class="fa fa-link" aria-hidden="true"></i></a>3.5. The Bellman Error is Not Learnable</h3>
<p>In reinforcement learning, a hypothesis is said to be ``learnable&apos;&apos; if it can be learned with any amount of experience. It turns out many quantities of apparent interest in reinforcement learning cannot be learned even from an infinite amount of experiential data. These quantities are well defined and can be computed given knowledge of the internal structure of the environment, but cannot be computed or estimated from the observed sequence of feature vectors, actions, and rewards.</p>
<p>There are counterexamples where the optimal parameter vector of <script type="math/tex; ">\overline{BE}</script> is not a function of the data and thus cannot be learned from it. However, another bootstrapping objective that we have considered, the <script type="math/tex; ">\overline{\mathrm{PBE}}</script>, can be determined from data (are learnable) and determine optimal solution that is in general different from the $\overline{\mathrm{BE}}$ minimums.</p>
<h3 id="gradient-td-methods"><a name="gradient-td-methods" class="anchor-navigation-ex-anchor" href="#gradient-td-methods"><i class="fa fa-link" aria-hidden="true"></i></a>3.6. Gradient-TD Methods</h3>
<p>Remember that in the linear case there is always an exact solution, the TD fixed point <script type="math/tex; ">w_{TD}</script>, at which the <script type="math/tex; ">\overline{\mathrm{PBE}}</script> is zero. This solution could be found by least-squares methods but only by methods of quadratic <script type="math/tex; ">O\left(d^{2}\right)</script> complexity in the number of parameters. We seek instead an SGD method.</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\overline{\operatorname{PBE}}(\mathbf{w})
&=\left\|\Pi \bar{\delta}_{\mathbf{w}}\right\|_{\mu}^{2} \\
&=\left(\Pi \bar{\delta}_{\mathbf{w}}\right)^{\top} \mathbf{D} \Pi \bar{\delta}_{\mathbf{w}} \\ &=\bar{\delta}_{\mathbf{w}}^{\top} \Pi^{\top} \mathbf{D} \Pi \bar{\delta}_{\mathbf{w}} \\
&=\bar{\delta}_{\mathbf{w}}^{\top} \mathbf{D} \mathbf{X}\left(\mathbf{X}^{\top} \mathbf{D} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{D} \bar{\delta}_{\mathbf{w}} \\
&=\left(\mathbf{X}^{\top} \mathbf{D} \bar{\delta}_{\mathbf{w}}\right)^{\top}\left(\mathbf{X}^{\top} \mathbf{D} \mathbf{X}\right)^{-1}\left(\mathbf{X}^{\top} \mathbf{D} \bar{\delta}_{\mathbf{w}}\right)
\end{aligned}
</script></p>
<p>The gradient with respect to <script type="math/tex; ">\mathbf{w}</script> is</p>
<p><script type="math/tex; mode=display">
\nabla \overline{\mathrm{PBE}}(\mathbf{w})=2 \nabla\left[\mathbf{X}^{\top} \mathbf{D} \bar{\delta}_{\mathbf{w}}\right]^{\top}\left(\mathbf{X}^{\top} \mathbf{D} \mathbf{X}\right)^{-1}\left(\mathbf{X}^{\top} \mathbf{D} \bar{\delta}_{\mathbf{w}}\right)
</script></p>
<p>To turn this into an SGD method, we have to sample something on every time step that has this quantity as its expected value. Notice that</p>
<p><script type="math/tex; mode=display">
\mathbf{X}^{\top} \mathbf{D} \bar{\delta}_{\mathbf{w}}=\sum_{s} \mu(s) \mathbf{x}(s) \bar{\delta}_{\mathbf{w}}(s)=\mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right]
</script></p>
<p>and</p>
<p><script type="math/tex; mode=display">
\begin{aligned} \nabla \mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right]^{\top} &=\mathbb{E}\left[\rho_{t} \nabla \delta_{t}^{\top} \mathbf{x}_{t}^{\top}\right] \\ &=\mathbb{E}\left[\rho_{t} \nabla\left(R_{t+1}+\gamma \mathbf{w}^{\top} \mathbf{x}_{t+1}-\mathbf{w}^{\top} \mathbf{x}_{t}\right)^{\top} \mathbf{x}_{t}^{\top}\right] \\ &=\mathbb{E}\left[\rho_{t}\left(\gamma \mathbf{x}_{t+1}-\mathbf{x}_{t}\right) \mathbf{x}_{t}^{\top}\right] \end{aligned}
</script></p>
<p>Finally,</p>
<p><script type="math/tex; mode=display">
\mathbf{X}^{\top} \mathbf{D} \mathbf{X}=\sum_{s} \mu(s) \mathbf{x}_{s} \mathbf{x}_{s}^{\top}=\mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]
</script></p>
<p>Substituting these expectations for the three factors in our expression,</p>
<p><script type="math/tex; mode=display">
\nabla \overline{\mathrm{PBE}}(\mathbf{w})=2 \mathbb{E}\left[\rho_{t}\left(\gamma \mathbf{x}_{t+1}-\mathbf{x}_{t}\right) \mathbf{x}_{t}^{\top}\right] \mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]^{-1} \mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right]
</script></p>
<p>Gradient-TD methods estimate and store the product of the second two factors above, denote as <script type="math/tex; ">\mathbf{v}</script>,</p>
<p><script type="math/tex; mode=display">
\mathbf{v} \approx \mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]^{-1} \mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right]
</script></p>
<p>This form is familiar to students of linear supervised learning. It is the solution to a linear
least-squares problem that tries to approximate <script type="math/tex; ">\rho_{t} \delta_{t}</script> from the features. The standard SGD method for incrementally finding the vector <script type="math/tex; ">\mathbf{v}</script> that minimizes the expected squared
error <script type="math/tex; ">\left(\mathbf{v}^{\top} \mathbf{x}_{t}-\rho_{t} \delta_{t}\right)^{2}</script> is known as the Least Mean Square (LMS) rule:</p>
<p><script type="math/tex; mode=display">
\mathbf{v}_{t+1} \doteq \mathbf{v}_{t}+\beta \rho_{t}\left(\delta_{t}-\mathbf{v}_{t}^{\top} \mathbf{x}_{t}\right) \mathbf{x}_{t}
</script></p>
<p>Therefore, the GTD2 algorithm is,</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+1}
&=\mathbf{w}_{t}-\frac{1}{2} \alpha \nabla \overline{\operatorname{PBE}}\left(\mathbf{w}_{t}\right) \\
&=\mathbf{w}_{t}-\frac{1}{2} \alpha 2 \mathbb{E}\left[\rho_{t}\left(\gamma \mathbf{x}_{t+1}-\mathbf{x}_{t}\right) \mathbf{x}_{t}^{\top}\right] \mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]^{-1} \mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right] \\
&=\mathbf{w}_{t}+\alpha \mathbb{E}\left[\rho_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right) \mathbf{x}_{t}^{\top}\right] \mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]^{-1} \mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right] \\
&\approx \mathbf{w}_{t}+\alpha \mathbb{E}\left[\rho_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right) \mathbf{x}_{t}^{\top}\right] \mathbf{v}_{t} \\
& \approx \mathbf{w}_{t}+\alpha \rho_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right) \mathbf{x}_{t}^{\top} \mathbf{v}_{t}
\end{aligned}
</script></p>
<p>A slightly better algorithm can be derived by doing a few more analytic steps before substituting in <script type="math/tex; ">\mathbf{v}_t</script>, which is called GTD(0) algorithm:</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+1}
&=\mathbf{w}_{t}+\alpha \mathbb{E}\left[\rho_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right) \mathbf{x}_{t}^{\top}\right] \mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]^{-1} \mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right] \\
&=\mathbf{w}_{t}+\alpha\left(\mathbb{E}\left[\rho_{t} \mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]-\gamma \mathbb{E}\left[\rho_{t} \mathbf{x}_{t+1} \mathbf{x}_{t}^{\top}\right]\right) \mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]^{-1} \mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right] \\
&=\mathbf{w}_{t}+\alpha\left(\mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]-\gamma \mathbb{E}\left[\rho_{t} \mathbf{x}_{t+1} \mathbf{x}_{t}^{\top}\right]\right) \mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]^{-1} \mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right] \\
&=\mathbf{w}_{t}+\alpha\left(\mathbb{E}\left[\mathbf{x}_{t} \rho_{t} \delta_{t}\right]-\gamma \mathbb{E}\left[\rho_{t} \mathbf{x}_{t+1} \mathbf{x}_{t}^{\top}\right] \mathbb{E}\left[\mathbf{x}_{t} \mathbf{x}_{t}^{\top}\right]^{-1} \mathbb{E}\left[\rho_{t} \delta_{t} \mathbf{x}_{t}\right]\right) \\
&\approx \mathbf{w}_{t}+\alpha\left(\mathbb{E}\left[\mathbf{x}_{t} \rho_{t} \delta_{t}\right]-\gamma \mathbb{E}\left[\rho_{t} \mathbf{x}_{t+1} \mathbf{x}_{t}^{\top}\right] \mathbf{v}_{t}\right) \\
&\approx \mathbf{w}_{t}+\alpha \rho_{t}\left(\delta_{t} \mathbf{x}_{t}-\gamma \mathbf{x}_{t+1} \mathbf{x}_{t}^{\top} \mathbf{v}_{t}\right)
\end{aligned}
</script></p>
<h2 id="eligibility-traces"><a name="eligibility-traces" class="anchor-navigation-ex-anchor" href="#eligibility-traces"><i class="fa fa-link" aria-hidden="true"></i></a>4. Eligibility Traces</h2>
<h3 id="td-lambda"><a name="td-lambda" class="anchor-navigation-ex-anchor" href="#td-lambda"><i class="fa fa-link" aria-hidden="true"></i></a>4.1. TD-lambda</h3>
<p>Recall the n-step return,</p>
<p><script type="math/tex; mode=display">
G_{t: t+n} \doteq R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1} R_{t+n}+\gamma^{n} \hat{v}\left(S_{t+n}, \mathbf{w}_{t+n-1}\right), \quad 0 \leq t \leq T-n
</script></p>
<p>Define the <script type="math/tex; ">\lambda</script>-return as,</p>
<p><script type="math/tex; mode=display">
G_{t}^{\lambda} \doteq(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t: t+n}
</script></p>
<p>we can separate these post-termination terms from the main sum, yielding</p>
<p><script type="math/tex; mode=display">
G_{t}^{\lambda}=(1-\lambda) \sum_{n=1}^{T-t-1} \lambda^{n-1} G_{t: t+n}+\lambda^{T-t-1} G_{t}
</script></p>
<p>as indicated in the figures. This equation makes it clearer what happens when <script type="math/tex; ">\lambda=1</script>. In
this case the main sum goes to zero, and the remaining term reduces to the conventional return. Thus, for <script type="math/tex; ">\lambda=1</script>, updating according to the <script type="math/tex; ">\lambda</script>-return is a Monte Carlo algorithm. On the other hand, if <script type="math/tex; ">\lambda=0</script>, then the <script type="math/tex; ">\lambda</script>-return reduces to <script type="math/tex; ">G_{t: t+1}</script>, the one-step return.</p>
<p>We are now ready to define our first learning algorithm based on the <script type="math/tex; ">\lambda</script>-return: the
offline <script type="math/tex; ">\lambda</script>-return algorithm. As an offline algorithm, it makes no changes to the weight vector during the episode. Then, at the end of the episode, a whole sequence of offline
updates are made according to our usual semi-gradient rule, using the <script type="math/tex; ">\lambda</script>-return as the
target:</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha\left[G_{t}^{\lambda}-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right), \quad t=0, \ldots, T-1
</script></p>
<p>TD(<script type="math/tex; ">\lambda</script>) is one of the oldest and most widely used algorithms in reinforcement learning. TD (<script type="math/tex; ">\lambda</script>) improves over the offline <script type="math/tex; ">\lambda</script>-return algorithm in three ways. First it updates the weight vector on every step of an episode rather than only at the end, and thus its estimates may be better sooner. Second, its computations are equally distributed in time rather than all at the end of the episode. And third, it can be applied to
continuing problems rather than just to episodic problems. In this section we present the
semi-gradient version of TD(<script type="math/tex; ">\lambda</script>) with function approximation.</p>
<p>In <script type="math/tex; ">\mathrm{TD}(\lambda),</script> the eligibility trace vector is initialized to zero at the beginning of the episode, is incremented on each time step by the value gradient, and then fades away by <script type="math/tex; ">\gamma \lambda</script>:</p>
<p><script type="math/tex; mode=display">
\begin{array}{l}{\mathbf{z}_{-1} \doteq \mathbf{0}} \\ {\mathbf{z}_{t} \doteq \gamma \lambda \mathbf{z}_{t-1}+\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right), \quad 0 \leq t \leq T}\end{array}
</script></p>
<p>where <script type="math/tex; ">\gamma</script> is the discount rate and <script type="math/tex; ">\lambda</script> is the parameter introduced in the previous section, which we henceforth call the trace-decay parameter. The eligibility trace keeps track of which components of the weight vector have contributed, positively or negatively, to recent state valuations, where &quot;recent&quot; is defined in terms of <script type="math/tex; ">\gamma \lambda</script>.</p>
<p><img src="pic/RL_book/RL_31.png" alt=""></p>
<p>We also show below the backward or mechanistic view of TD(<script type="math/tex; ">\lambda</script>). Each update depends on the current TD error combined with the current eligibility traces of past events.</p>
<p><img src="pic/RL_book/RL_32.png" alt=""></p>
<p>Linear <script type="math/tex; ">\mathrm{TD}(\lambda)</script> has been proved to converge in the on-policy case if the step-size parameter is reduced over time according to the usual conditions. For the continuing discounted case,</p>
<p><script type="math/tex; mode=display">
\overline{\mathrm{VE}}\left(\mathbf{w}_{\infty}\right) \leq \frac{1-\gamma \lambda}{1-\gamma} \min _{\mathbf{w}} \overline{\mathrm{VE}}(\mathbf{w})
</script></p>
<p>In practice, however, <script type="math/tex; ">\lambda=1</script> is often the poorest choice.</p>
<h3 id="online-td-lambda"><a name="online-td-lambda" class="anchor-navigation-ex-anchor" href="#online-td-lambda"><i class="fa fa-link" aria-hidden="true"></i></a>4.2. Online TD-lambda</h3>
<p>In general, we define the truncated <script type="math/tex; ">\lambda</script>-return for time <script type="math/tex; ">t</script>, given data only up to some later horizon, <script type="math/tex; ">h</script>, as</p>
<p><script type="math/tex; mode=display">
G_{t:h}^{\lambda} \doteq(1-\lambda) \sum_{n=1}^{h-t-1} \lambda^{n-1} G_{t: t+n}+\lambda^{h-t-1} G_{t:h}, \quad 0 \leq t<h \leq T
</script></p>
<p>The conceptual algorithm involves multiple passes over the episode, one at each horizon, each generating a different sequence of weight vectors. Let us use <script type="math/tex; ">w_t^h</script> to denote the weights used to generate the value at time <script type="math/tex; ">t</script> in the sequence up to horizon <script type="math/tex; ">h</script>. The first weight vector <script type="math/tex; ">w_0^h</script> in each sequence is that inherited from the previous episode.</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
h=1: \quad &\mathbf{w}_{1}^{1} \doteq \mathbf{w}_{0}^{1}+\alpha\left[G_{0: 1}^{\lambda}-\hat{v}\left(S_{0}, \mathbf{w}_{0}^{1}\right)\right] \nabla \hat{v}\left(S_{0}, \mathbf{w}_{0}^{1}\right) \\
 \\
h=2: \quad & \mathbf{w}_{1}^{2} \doteq \mathbf{w}_{0}^{2}+\alpha\left[G_{0: 2}^{\lambda}-\hat{v}\left(S_{0}, \mathbf{w}_{0}^{2}\right)\right] \nabla \hat{v}\left(S_{0}, \mathbf{w}_{0}^{2}\right) \\
& \mathbf{w}_{2}^{2} \doteq \mathbf{w}_{1}^{2}+\alpha\left[G_{1: 2}^{\lambda}-\hat{v}\left(S_{1}, \mathbf{w}_{1}^{2}\right)\right] \nabla \hat{v}\left(S_{1}, \mathbf{w}_{1}^{2}\right) \\
 \\
h=3: \quad & \mathbf{w}_{1}^{3} \doteq \mathbf{w}_{0}^{3}+\alpha\left[G_{0: 3}^{\lambda}-\hat{v}\left(S_{0}, \mathbf{w}_{0}^{3}\right)\right] \nabla \hat{v}\left(S_{0}, \mathbf{w}_{0}^{3}\right) \\ & \mathbf{w}_{2}^{3} \doteq \mathbf{w}_{1}^{3}+\alpha\left[G_{1: 3}^{\lambda}-\hat{v}\left(S_{1}, \mathbf{w}_{1}^{3}\right)\right] \nabla \hat{v}\left(S_{1}, \mathbf{w}_{1}^{3}\right) \\ & \mathbf{w}_{3}^{3} \doteq \mathbf{w}_{2}^{3}+\alpha\left[G_{2: 3}^{\lambda}-\hat{v}\left(S_{2}, \mathbf{w}_{2}^{3}\right)\right] \nabla \hat{v}\left(S_{2}, \mathbf{w}_{2}^{3}\right)
\end{aligned}
</script></p>
<p>The general form for the update is</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t+1}^{h} \doteq \mathbf{w}_{t}^{h}+\alpha\left[G_{t: h}^{\lambda}-\hat{v}\left(S_{t}, \mathbf{w}_{t}^{h}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}^{h}\right), \quad 0 \leq t<h \leq T
</script></p>
<p>Now we are ready to introduce the true online TD(<script type="math/tex; ">\lambda</script>). The sequence of weight vectors produced by the online <script type="math/tex; ">\lambda</script>-return algorithm can be arranged in a triangle:</p>
<p><script type="math/tex; mode=display">
\begin{array}{cccccc}{\mathbf{w}_{0}^{0}} & {} & {} & {} & {} \\ {\mathbf{w}_{0}^{1}} & {\mathbf{w}_{1}^{1}} & {} & {} \\ {\mathbf{w}_{0}^{2}} & {\mathbf{w}_{1}^{2}} & {\mathbf{w}_{2}^{2}} & {} \\ {\mathbf{w}_{0}^{3}} & {\mathbf{w}_{1}^{3}} & {\mathbf{w}_{2}^{3}} & {\mathbf{w}_{3}^{3}} & {} \\ {} & {\vdots} & {\vdots} & {\vdots} & {\vdots} & {\ddots} \\ {\mathbf{w}_{0}^{T}} & {\mathbf{w}_{1}^{T}} & {\mathbf{w}_{2}^{T}} & {\mathbf{w}_{3}^{T}} & {\cdots} & {\mathbf{w}_{T}^{T}}\end{array}
</script></p>
<p>One row of this triangle is produced on each time step. It turns out that the weight
vectors on the diagonal, the <script type="math/tex; ">w_t^t</script>, are the only ones really needed. The strategy then is to find a compact, efficient way of computing each <script type="math/tex; ">w_t^t</script> from the one before. Pseudocode for the complete algorithm is given in the box.</p>
<p><img src="pic/RL_book/RL_33.png" alt=""></p>
<p>We can define the action-value method (Sarsa(<script type="math/tex; ">\lambda</script>)) similarly. The Sarsa(<script type="math/tex; ">\lambda</script>) with binary features and linear function approximation is presented below.</p>
<p><img src="pic/RL_book/RL_34.png" alt=""></p>
<p><img src="pic/RL_book/RL_35.png" alt=""></p>
<h3 id="off-policy-traces-with-control-variates"><a name="off-policy-traces-with-control-variates" class="anchor-navigation-ex-anchor" href="#off-policy-traces-with-control-variates"><i class="fa fa-link" aria-hidden="true"></i></a>4.3. Off-policy Traces with Control Variates</h3>
<p>In the state case, we incorporate importance sampling with TD(<script type="math/tex; ">\lambda</script>).</p>
<p><script type="math/tex; mode=display">
G_{t}^{\lambda} \doteq \rho_{t}\left(R_{t+1}+\gamma\left(\left(1-\lambda\right) \hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)+\lambda G_{t+1}^{\lambda s}\right)\right)+\left(1-\rho_{t}\right) \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
</script></p>
<p>the truncated version of this return can be approximated simply in terms of sums of the state-based TD-error</p>
<p><script type="math/tex; mode=display">
\delta_{t}^{s} \doteq R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
</script></p>
<p>and</p>
<p><script type="math/tex; mode=display">
G_{t}^{\lambda} \approx \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)+\rho_{t} \sum_{k=t}^{\infty} \delta_{k}^{s} \prod_{i=t+1}^{k} \gamma \lambda \rho_{i}
</script></p>
<p>Therefore,</p>
<p><script type="math/tex; mode=display">
\begin{aligned} \mathbf{w}_{t+1} &=\mathbf{w}_{t}+\alpha\left(G_{t}^{\lambda}-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right) \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \\ & \approx \mathbf{w}_{t}+\alpha \rho_{t}\left(\sum_{k=t}^{\infty} \delta_{k}^{s} \prod_{i=t+1}^{k} \gamma \lambda \rho_{i}\right) \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \end{aligned}
</script></p>
<p>The sum of the forward-view update over time is</p>
<p><script type="math/tex; mode=display">
\begin{aligned} \sum_{t=1}^{\infty}\left(\mathbf{w}_{t+1}-\mathbf{w}_{t}\right) & \approx \sum_{t=1}^{\infty} \sum_{k=t}^{\infty} \alpha \rho_{t} \delta_{k}^{s} \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \prod_{i=t+1}^{k} \gamma \lambda \rho_{i} \\ &=\sum_{k=1}^{\infty} \sum_{t=1}^{k} \alpha \rho_{t} \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \delta_{k}^{s} \prod_{i=t+1}^{k} \gamma \lambda \rho_{i} \\
&=\sum_{k=1}^{\infty} \alpha \delta_{k}^{s} \sum_{t=1}^{k} \rho_{t} \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \prod_{i=t+1}^{k} \gamma \lambda \rho_{i}
\end{aligned}
</script></p>
<p>which would be in the form of the sum of a backward-view TD update if the entire expression from the second sum left could be written and updated incrementally as an eligibility trace.</p>
<p><script type="math/tex; mode=display">
\begin{aligned} \mathbf{z}_{k} &=\sum_{t=1}^{k} \rho_{t} \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \prod_{i=t+1}^{k} \gamma \lambda \rho_{i} \\ &=\sum_{t=1}^{k-1} \rho_{t} \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \prod_{i=t+1}^{k} \gamma \lambda \rho_{i}+\rho_{k} \nabla \hat{v}\left(S_{k}, \mathbf{w}_{k}\right) \\ &=\gamma_{k} \lambda_{k} \rho_{k} \sum_{t=1}^{k-1} \rho_{t} \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \prod_{i=t+1}^{k-1} \gamma \lambda \rho_{i} \; + \; \rho_{k} \nabla \hat{v}\left(S_{k}, \mathbf{w}_{k}\right) \\
&=\rho_{k}\left(\gamma_{k} \lambda_{k} \mathbf{z}_{k-1}+\nabla \hat{v}\left(S_{k}, \mathbf{w}_{k}\right)\right)
\end{aligned}
</script></p>
<h3 id="eligibility-traces-q-learning"><a name="eligibility-traces-q-learning" class="anchor-navigation-ex-anchor" href="#eligibility-traces-q-learning"><i class="fa fa-link" aria-hidden="true"></i></a>4.4. Eligibility Traces Q-learning</h3>
<p>The concept of TB(<script type="math/tex; ">\lambda</script>) is straightforward.</p>
<p><script type="math/tex; mode=display">
\begin{aligned} G_{t}^{\lambda} & \doteq R_{t+1}+\gamma\left(\left(1-\lambda\right) \bar{V}_{t}\left(S_{t+1}\right)+\lambda\left[\sum_{a \neq A_{t+1}} \pi\left(a \; | \; S_{t+1}\right) \hat{q}\left(S_{t+1}, a, \mathbf{w}_{t}\right)+\pi\left(A_{t+1} \; | \; S_{t+1}\right) G_{t+1}^{\lambda}\right]\right) \\ &=R_{t+1}+\gamma\left(\bar{V}_{t}\left(S_{t+1}\right)+\lambda \pi\left(A_{t+1} \; | \; S_{t+1}\right)\left(G_{t+1}^{\lambda}-\hat{q}\left(S_{t+1}, A_{t+1}, \mathbf{w}_{t}\right)\right)\right) \end{aligned}
</script></p>
<p>As per the usual pattern, it can also be written approximately as a sum of TD errors.</p>
<p><script type="math/tex; mode=display">
G_{t}^{\lambda} \approx \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right)+\sum_{k=t}^{\infty} \delta_{k}^{a} \prod_{i=t+1}^{k} \gamma \lambda \pi\left(A_{i} \; |\; S_{i}\right)
</script></p>
<h3 id="stable-off-policy-methods-with-traces"><a name="stable-off-policy-methods-with-traces" class="anchor-navigation-ex-anchor" href="#stable-off-policy-methods-with-traces"><i class="fa fa-link" aria-hidden="true"></i></a>4.5. Stable Off-policy Methods with Traces</h3>
<p>Several methods using eligibility traces have been proposed that achieve guarantees of stability under off-policy training, and here we present two of the most important ones.</p>
<p><strong><em>GTD(<script type="math/tex; ">\lambda</script>)</em></strong> is the eligibility-trace algorithm analogous to GTD(0) the better of the two state-value Gradient-TD prediction algorithms discussed above. Its goal is to learn a parameter <script type="math/tex; ">\mathbf{w}_{t}</script> such that <script type="math/tex; ">\hat{v}(s, \mathbf{w}) \stackrel{\vdots}{=} \mathbf{w}_{t}^{\top} \mathbf{x}(s) \approx v_{\pi}(s)</script>, even from data that is due to following another policy <script type="math/tex; ">b</script>. Its update is</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \delta_{t}^{s} \mathbf{z}_{t}-\alpha \gamma\left(1-\lambda\right)\left(\mathbf{z}_{t}^{\top} \mathbf{v}_{t}\right) \mathbf{x}_{t+1}
</script></p>
<p>where</p>
<p><script type="math/tex; mode=display">
\mathbf{v}_{t+1} \doteq \mathbf{v}_{t}+\beta \delta_{t}^{s} \mathbf{z}_{t}-\beta\left(\mathbf{v}_{t}^{\top} \mathbf{x}_{t}\right) \mathbf{x}_{t}
</script></p>
<p><strong><em>GQ<script type="math/tex; ">(\lambda)</script></em></strong> is the Gradient-TD algorithm for action values with eligibility traces. Its goal is to learn a parameter <script type="math/tex; ">\mathbf{w}_{t}</script> such that <script type="math/tex; ">\hat{q}\left(s, a, \mathbf{w}_{t}\right) \doteq \mathbf{w}_{t}^{\top} \mathbf{x}(s, a) \approx q_{\pi}(s, a)</script> from off-policy. Its update is</p>
<p><script type="math/tex; mode=display">
\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \delta_{t}^{a} \mathbf{z}_{t}-\alpha \gamma\left(1-\lambda\right)\left(\mathbf{z}_{t}^{\top} \mathbf{v}_{t}\right) \overline{\mathbf{x}}_{t+1}
</script></p>
<p>where <script type="math/tex; ">\overline{\mathbf{x}}_{t}</script> is the average feature vector for <script type="math/tex; ">S_{t}</script> under the target policy, <script type="math/tex; ">\delta_{t}^{a}</script> is the expectation form of the TD error,</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\overline{\mathbf{x}}_{t} &\doteq \sum_{a} \pi\left(a | S_{t}\right) \mathbf{x}\left(S_{t}, a\right) \\
\delta_{t}^{a} &\doteq R_{t+1}+\gamma \mathbf{w}_{t}^{\top} \overline{\mathbf{x}}_{t+1}-\mathbf{w}_{t}^{\top} \mathbf{x}_{t} \\
\mathbf{z}_{t} &\doteq \gamma \lambda \rho_{t} \mathbf{z}_{t-1}+\nabla \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right)
\end{aligned}
</script></p>
<p><strong><em>HTD<script type="math/tex; ">(\lambda)</script></em></strong> is a hybrid state-value algorithm combining aspects of GTD $(\lambda)$ and <script type="math/tex; ">\mathrm{TD}(\lambda)</script>. Its most appealing feature is that it is a strict generalization of <script type="math/tex; ">\mathrm{TD}(\lambda)</script> to off-policy learning, meaning that if the behavior policy happens to be the same as the target policy, then <script type="math/tex; ">\mathrm{HTD}(\lambda)</script> becomes the same as <script type="math/tex; ">\mathrm{TD}(\lambda)</script>, which is not true for <script type="math/tex; ">\mathrm{GTD}(\lambda)</script>. HTD<script type="math/tex; ">(\lambda)</script> is defined as</p>
<p><script type="math/tex; mode=display">
\begin{aligned} \mathbf{w}_{t+1} & \doteq \mathbf{w}_{t}+\alpha \delta_{t}^{s} \mathbf{z}_{t}+\alpha\left(\left(\mathbf{z}_{t}-\mathbf{z}_{t}^{b}\right)^{\top} \mathbf{v}_{t}\right)\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right) \\ \mathbf{v}_{t+1} & \doteq \mathbf{v}_{t}+\beta \delta_{t}^{s} \mathbf{z}_{t}-\beta\left(\mathbf{z}_{t}^{b} \mathbf{v}_{t}\right)\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right), \quad \text { with } \mathbf{v}_{0} \doteq \mathbf{0} \\ \mathbf{z}_{t} & \doteq \rho_{t}\left(\gamma \lambda \mathbf{z}_{t-1}+\mathbf{x}_{t}\right), \quad \text { with } \mathbf{z}_{-1} \doteq \mathbf{0} \\ \mathbf{z}_{t}^{b} & \doteq \gamma \lambda \mathbf{z}_{t-1}^{b}+\mathbf{x}_{t}, \quad \text { with } \mathbf{z}_{-1}^{b} \doteq \mathbf{0} \end{aligned}
</script></p>
<h2 id="policy-gradient-methods"><a name="policy-gradient-methods" class="anchor-navigation-ex-anchor" href="#policy-gradient-methods"><i class="fa fa-link" aria-hidden="true"></i></a>5. Policy Gradient Methods</h2>
<h3 id="policy-approximation-and-the-policy-gradient-theorem"><a name="policy-approximation-and-the-policy-gradient-theorem" class="anchor-navigation-ex-anchor" href="#policy-approximation-and-the-policy-gradient-theorem"><i class="fa fa-link" aria-hidden="true"></i></a>5.1. Policy Approximation and the Policy Gradient Theorem</h3>
<p>In this chapter we consider methods for learning the policy parameter based on the gradient of some scalar performance measure <script type="math/tex; ">J(\boldsymbol{\theta})</script> with respect to the policy parameter. These methods seek to maximize performance, so their updates stochastic gradient ascent in <script type="math/tex; ">J</script> is</p>
<p><script type="math/tex; mode=display">
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\alpha \widehat{\nabla J\left(\boldsymbol{\theta}_{t}\right)}
</script></p>
<p>In policy gradient methods, the policy can be parameterized in any way, as long as <script type="math/tex; ">\pi(a | s, \boldsymbol{\theta})</script> is differentiable with respect to its parameters.</p>
<p>If the action space is discrete and not too large, then a natural and common kind of parameterization is to form parameterized numerical preferences <script type="math/tex; ">h(s, a, \boldsymbol{\theta}) \in \mathbb{R}</script> and the actions with the highest preferences in each state are given the highest probabilities of being selected. For example,</p>
<p><script type="math/tex; mode=display">
\pi(a | s, \boldsymbol{\theta}) \doteq \frac{e^{h(s, a, \boldsymbol{\theta})}}{\sum_{b} e^{h(s, b, \boldsymbol{\theta})}}
</script></p>
<p>The action preferences themselves can be parameterized arbitrarily. For example, they
might be computed by a deep artificial neural network, where <script type="math/tex; ">\boldsymbol{\theta}</script> is the vector of all the connection weights of the network. Or the preferences could simply be linear in features,</p>
<p><script type="math/tex; mode=display">
h(s, a, \boldsymbol{\theta})=\boldsymbol{\theta}^{\top} \mathbf{x}(s, a)
</script></p>
<p>In the episodic case we define performance as</p>
<p><script type="math/tex; mode=display">
J(\boldsymbol{\theta}) \doteq v_{\pi_{\theta}}\left(s_{0}\right)
</script></p>
<p>There is an excellent theoretical answer to this challenge in the form of the policy gradient theorem, which provides an analytic expression for the gradient of performance with respect to the policy parameter. The policy gradient theorem for the episodic case establishes that</p>
<p><script type="math/tex; mode=display">
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta})
</script></p>
<p>The proof of the policy gradient theorem is given below.</p>
<p><img src="pic/RL_book/RL_36.png" alt=""></p>
<h3 id="reinforce-monte-carlo-policy-gradient"><a name="reinforce-monte-carlo-policy-gradient" class="anchor-navigation-ex-anchor" href="#reinforce-monte-carlo-policy-gradient"><i class="fa fa-link" aria-hidden="true"></i></a>5.2. REINFORCE: Monte Carlo Policy Gradient</h3>
<p>Notice that</p>
<p><script type="math/tex; mode=display">
\begin{aligned}
\nabla J(\boldsymbol{\theta})
&\propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta}) \\
&=\mathbb{E}_{\pi}\left[\sum_{a} q_{\pi}\left(S_{t}, a\right) \nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)\right] \\
&=\mathbb{E}_{\pi}\left[\sum_{a} \pi\left(a | S_{t}, \boldsymbol{\theta}\right) q_{\pi}\left(S_{t}, a\right) \frac{\nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(a | S_{t}, \boldsymbol{\theta}\right)}\right] \\
&=\mathbb{E}_{\pi}\left[q_{\pi}\left(S_{t}, A_{t}\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}\right] \\
&=\mathbb{E}_{\pi}\left[G_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}\right]
\end{aligned}
</script></p>
<p>where we introduce <script type="math/tex; ">A_t</script> and replace a sum over the random variable&#x2019;s possible values by an expectation under <script type="math/tex; ">\pi</script>, and then sampling the expectation.</p>
<p>Using this sample to instantiate our generic stochastic gradient ascent algorithm yields the REINFORCE update:</p>
<p><script type="math/tex; mode=display">
\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha G_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}
</script></p>
<p>where <script type="math/tex; ">G_t</script> is obtained by Monte-Carlo sampling.</p>
<p><img src="pic/RL_book/RL_37.png" alt=""></p>
<h3 id="reinforce-with-baseline"><a name="reinforce-with-baseline" class="anchor-navigation-ex-anchor" href="#reinforce-with-baseline"><i class="fa fa-link" aria-hidden="true"></i></a>5.3. REINFORCE with Baseline</h3>
<p>The policy gradient theorem can be generalized to include a comparison of the action value to an arbitrary baseline <script type="math/tex; ">b(s)</script>:</p>
<p><script type="math/tex; mode=display">
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a}\left(q_{\pi}(s, a)-b(s)\right) \nabla \pi(a | s, \boldsymbol{\theta})
</script></p>
<p>since the baseline can be any function, even a random variable, as long as it does not vary with <script type="math/tex; ">a</script></p>
<p><script type="math/tex; mode=display">
\sum_{a} b(s) \nabla \pi(a | s, \boldsymbol{\theta})=b(s) \nabla \sum_{a} \pi(a | s, \boldsymbol{\theta})=b(s) \nabla 1=0
</script></p>
<p>The policy gradient theorem with baseline can be used to derive an update rule using similar steps as in the previous section. The update rule that we end up with is a new version of REINFORCE that includes a general baseline:</p>
<p><script type="math/tex; mode=display">
\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha\left(G_{t}-b\left(S_{t}\right)\right) \frac{\nabla \pi\left(A_{t}\; | \; S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t}\; | \; S_{t}, \boldsymbol{\theta}_{t}\right)}
</script></p>
<p>Because the baseline could be uniformly zero, this update is a strict generalization of REINFORCE. In general, the baseline leaves the expected value of the update unchanged, but it can have a large effect on its variance. For example, in the bandit algorithms the baseline was just a number (the average of the rewards seen so far), but for MDPs the baseline should vary with state. In some states all actions have high values and we need a high baseline to differentiate the higher valued actions from the less highly valued ones; in other states all actions will have low values and a low baseline is appropriate.</p>
<p>One natural choice for the baseline is an estimate of the state value, <script type="math/tex; ">\hat{v}\left(S_{t}, \mathbf{w}\right)</script>, where <script type="math/tex; ">\mathbf{w} \in \mathbb{R}^{m}</script> is a weight vector learned by one of the methods presented in previous chapters.</p>
<h3 id="actor&#x2013;critic-methods"><a name="actor&#x2013;critic-methods" class="anchor-navigation-ex-anchor" href="#actor&#x2013;critic-methods"><i class="fa fa-link" aria-hidden="true"></i></a>5.4. Actor&#x2013;Critic Methods</h3>
<p>Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor&#x2013;critic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating the value estimate for a state from the estimated values of subsequent states), but only as a baseline for the state whose estimate is being updated. This is a useful distinction, for only through bootstrapping do we introduce bias and an asymptotic dependence on the quality of the function approximation. As we have seen, the bias introduced through bootstrapping and reliance on the state representation is often beneficial because it reduces variance and accelerates learning. REINFORCE with baseline is unbiased and will converge asymptotically to a local minimum, but like all Monte Carlo methods it tends to learn slowly (produce estimates of high variance) and to be inconvenient to implement online or for continuing problems. As we have seen earlier in this book, with temporal-difference methods we can eliminate these inconveniences, and through multi-step methods we can flexibly choose the degree of bootstrapping. In order to gain these advantages in the case of policy gradient methods we use actor&#x2013;critic methods with a bootstrapping critic.</p>
<p>First consider one-step actor&#x2013;critic methods. The main appeal of one-step methods is that they are fully online and incremental, yet avoid the complexities of eligibility traces.</p>
<p><script type="math/tex; mode=display">
\begin{aligned} \boldsymbol{\theta}_{t+1} & \doteq \boldsymbol{\theta}_{t}+\alpha\left(G_{t: t+1}-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t}\; | \; S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t}\; | \; S_{t}, \boldsymbol{\theta}_{t}\right)} \\ &=\boldsymbol{\theta}_{t}+\alpha\left(R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}\right)-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t}\; | \; S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t}\; | \; S_{t}, \boldsymbol{\theta}_{t}\right)} \\ &=\boldsymbol{\theta}_{t}+\alpha \delta_{t} \frac{\nabla \pi\left(A_{t}\; | \; S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t}\; | \; S_{t}, \boldsymbol{\theta}_{t}\right)} \end{aligned}
</script></p>
<p>The natural state-value-function learning method to pair with this is semi-gradient TD(0). The pseudocode is shown below.</p>
<p><img src="pic/RL_book/RL_38.png" alt=""></p>
<p>The generalizations to the forward view of n-step methods and then to a <script type="math/tex; ">\lambda</script>-return algorithm are straightforward.</p>
<p><img src="pic/RL_book/RL_39.png" alt=""></p>
<h3 id="policy-gradient-for-continuing-problems"><a name="policy-gradient-for-continuing-problems" class="anchor-navigation-ex-anchor" href="#policy-gradient-for-continuing-problems"><i class="fa fa-link" aria-hidden="true"></i></a>5.5. Policy Gradient for Continuing Problems</h3>
<p>As discussed previously, for continuing problems without episode boundaries we need to define performance in terms of the average rate of reward per time step:</p>
<p><script type="math/tex; mode=display">
\begin{aligned} J(\boldsymbol{\theta}) \doteq r(\pi) & \doteq \lim _{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}\left[R_{t} | S_{0}, A_{0: t-1} \sim \pi\right] \\ &=\lim _{t \rightarrow \infty} \mathbb{E}\left[R_{t} | S_{0}, A_{0: t-1} \sim \pi\right] \\ &=\sum_{s} \mu(s) \sum_{a} \pi(a | s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right) r \end{aligned}
</script></p>
<p>where <script type="math/tex; ">\mu</script> is the steady-state distribution under <script type="math/tex; ">\pi, \mu(s) \doteq \lim _{t \rightarrow \infty} \operatorname{Pr}\left\{S_{t}=s \; | \; A_{0: t} \sim \pi\right\}</script> which is assumed to exist and to be independent of <script type="math/tex; ">S_{0}</script> (an ergodicity assumption).</p>
<p><script type="math/tex; mode=display">
\sum_{s} \mu(s) \sum_{a} \pi(a | s, \boldsymbol{\theta}) p\left(s^{\prime} | s, a\right)=\mu\left(s^{\prime}\right),$ for all $s^{\prime} \in \mathcal{S}
</script></p>
<p>Naturally, in the continuing case, we define values, <script type="math/tex; ">v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right]</script> and <script type="math/tex; ">q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s, A_{t}=a\right]</script>, with respect to the differential return:</p>
<p><script type="math/tex; mode=display">
G_{t} \doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\cdots
</script></p>
<p>With these alternate definitions, the policy gradient theorem as given for the episodic case remains true for the continuing case.</p>
<p><img src="pic/RL_book/RL_40.png" alt=""></p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="RL_Book1.html" class="navigation navigation-prev " aria-label="Previous page: Part 1: Reinforcement Learning: An Introduction">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="TRPO.html" class="navigation navigation-next " aria-label="Next page: Trust Region Policy Optimization">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Part 2: Reinforcement Learning: An Introduction (2)","level":"1.4.18","depth":2,"next":{"title":"Trust Region Policy Optimization","level":"1.4.19","depth":2,"path":"notes/TRPO.md","ref":"./notes/TRPO.md","articles":[]},"previous":{"title":"Part 1: Reinforcement Learning: An Introduction","level":"1.4.17","depth":2,"path":"notes/RL_Book1.md","ref":"./notes/RL_Book1.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax","image-captions","github","anchors","anchor-navigation-ex","-sharing","sharing-plus@^0.0.2","github-buttons","embed-pdf","livereload"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"github":{"url":"https://github.com/ZizouHe/"},"livereload":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"fa fa-hand-o-right","level2Icon":"fa fa-hand-o-right","level3Icon":"fa fa-hand-o-right","showLevelIcon":false},"mode":"float","multipleH1":false,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":true},"github-buttons":{"buttons":[{"user":"ZizouHe","repo":"reading-notes","type":"star","size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"qq":true,"all":["facebook","google","twitter","weibo","qq","linkedin"],"douban":false,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":false,"messenger":false,"line":false,"vk":false,"pocket":false,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"anchors":{},"embed-pdf":{},"image-captions":{"caption":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","align":"right","variable_name":"_pictures"}},"theme":"default","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{"_pictures":[{"backlink":"./notes/RDN.html#fig1.4.2.1","level":"1.4.2","align":"right","list_caption":"Figure: Example (a) data graph and (b) model graph.","alt":"Example (a) data graph and (b) model graph.","nro":1,"url":"./pic/RDN-1.png","index":1,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Example (a) data graph and (b) model graph.","attributes":{},"skip":false,"key":"1.4.2.1"},{"backlink":"./notes/RDN.html#fig1.4.2.2","level":"1.4.2","align":"right","list_caption":"Figure: RDN inference algorithm.","alt":"RDN inference algorithm.","nro":2,"url":"./pic/RDN-2.png","index":2,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"RDN inference algorithm.","attributes":{},"skip":false,"key":"1.4.2.2"},{"backlink":"./notes/Faster_Frank_Wolfe.html#fig1.4.14.1","level":"1.4.14","align":"right","list_caption":"Figure: Frank-Wolfe Algorithm","alt":"Frank-Wolfe Algorithm","nro":3,"url":"./pic/Frank-Wolfe.png","index":1,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Frank-Wolfe Algorithm","attributes":{},"skip":false,"key":"1.4.14.1"},{"backlink":"./notes/Fin_Fan_Book.html#fig1.4.15.1","level":"1.4.15","align":"right","list_caption":"Figure: Efficient Frontier with risk-free asset","alt":"Efficient Frontier with risk-free asset","nro":4,"url":"./pic/Eff_Front_1.png","index":1,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Efficient Frontier with risk-free asset","attributes":{},"skip":false,"key":"1.4.15.1"},{"backlink":"./notes/Fin_Fan_Book.html#fig1.4.15.2","level":"1.4.15","align":"right","list_caption":"Figure: Efficient Frontier without risk-free asset","alt":"Efficient Frontier without risk-free asset","nro":5,"url":"./pic/Eff_Front_2.png","index":2,"caption_template":"Figure _PAGE_IMAGE_NUMBER_. _CAPTION_","label":"Efficient Frontier without risk-free asset","attributes":{},"skip":false,"key":"1.4.15.2"}]},"title":"Reading List & Notes","gitbook":"*"},"file":{"path":"notes/RL_Book2.md","mtime":"2022-10-26T16:39:28.915Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2025-04-11T03:50:04.773Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

