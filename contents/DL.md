# Deep Learning

## Architechture

1. Dynamic Routing Between Capsules
2. Matrix Capsules with Em Routing
3. Deep Residual Learning for Image Recognition
4. [Neural Ordinary Differential Equations](../notes/NeuralODE.html)

## Deep Causal Inference

1. Causal Effect Inference with Deep Latent-Variable Models
2. The Information Bottleneck Method
3. Causal Deep Information Bottleneck

## Deep Reinforcement Learning

1. Mastering the game of Go with deep neural networks and tree search

## Generative Adversarial Network

1. Generative Adversarial Nets
2. towards principled methods for training generative adversarial networks
3. Wasserstein GAN

## Generalization

1. Dark Knowledge
2. Distilling the Knowledge in a Neural Network 
3. Understanding Black-box Predictions via Influence Functions
4. Understanding Deep Learning Requires Rethinking Generalization
5. DeepFool: a simple and accurate method to fool deep neural networks
6. [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](../notes/Sharpe_Minima_Exp.html)
7. [Sharp Minima Can Generalize For Deep Nets](../notes/Sharpe_Minima_Works.html)


## Measuring Uncertainty

1. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
2. On Calibration of Modern Neural Networks
3. Predicting Good Probabilities With Supervised Learning 
4. A Comprehensive Review of Neural Network-based Prediction Intervals and New Advances
5. Lower Upper Bound Estimation Method for Construction of Neural Network-Based Prediction
6. Estimating the Mean and Variance of the Target Probability Distribution
7. Wind Power Interval Prediction Based on Improved PSO and BP Neural Network
8. Prediction intervals for artificial neural networks
9. Practical confidence and prediction intervals

## Natural Language Processing

1. Teaching Machines to Read and Comprehend
2. Using the output embedding to improve language models
3. Tying word vectors and word classifiers: A loss framework for language modeling

## Training Technique

1. Improving neural networks by preventing co-adaptation of feature detectors
2. Dropout: A simple way to prevent neural networks from overfitting
3. An empirical analysis of dropout in piecewise linear networks
4. Understanding Dropout
5. Asynchronous Stochastic Gradient Descent with Delay Compensation
6. Understanding Synthetic Gradients and Decoupled Neural Interfaces
7. Decoupled Neural Interfaces using Synthetic Gradients
8. Batch Normalization- Accelerating Deep Network Training by Reducing Internal Covariate Shift

## Transfer Learning

1. How transferable are features in deep neural networks?