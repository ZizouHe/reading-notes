# Numerical and Optimization

## Numerical Computation

1. The Evaluation of Integrals of the form $$\int^{+\infty}_{-\infty} f(t)\exp^{−t^2} dt$$: Application to Logistic-Normal Models
2. Faster Eigenvector Computation via Shift-and-Invert Preconditioning

## Generalization

1. Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability

## Optimization

1. A Differential Equation for Modeling Nesterov’s Accelerated Gradient Method
2. SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient
3. Accelerating Stochastic Gradient Descent using Predictive Variance Reduction
4. Semi-Stochastic Gradient Descent Methods
5. [Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets](../notes/Faster_Frank_Wolfe.html)
6. Spider: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator
7. How to Escape Saddle Point Efficiently?

## Phase Retrieval/Single Index Model

1. Online Stochastic Gradient Descent with Arbitrary Initialization Solves Non-smooth, Non-convex Phase Retrieval
2. Phase Retrieval via Randomized Kaczmarz: Theoretical Guarantees
3. Phase Retrieval via Wirtinger Flow: Theory and Algorithms
4. Efficient Learning of Generalized Linear and Single Index Models

## Principal Component Analysis

1. Fast and Simple PCA via Convex Optimization
2. Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation
3. Faster Eigenvector Computation via Shift-and-Invert Preconditioning
4. LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain
5. Fast Stochastic Algorithms for SVD and PCA- Convergence Properties and Convexity

## Stochastic Gradient Langevin Dynamics

1. Non-convex Learning via Stochastic Gradient Langevin Dynamics: A Nonasymptotic Analysis
2. Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability

## Other SGD Method

1. SignSGD: Compressed Optimisation for Non-Convex Problems
2. SignSGD via Zeroth-Order Oracle