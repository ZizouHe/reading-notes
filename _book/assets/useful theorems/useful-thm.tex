\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}

% Definitions of handy macros can go here
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{amsmath,amssymb,amsthm,amsfonts,amsbsy, epsfig, psfrag,epsf}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\I}{\boldsymbol{I}}
\newcommand{\intd}{\, \mathrm{d}}
\newcommand{\D}{{\cal D}}
\renewcommand{\O}{{\mathcal{O}}}
\renewcommand{\a}{\boldsymbol{a}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\z}{\boldsymbol{z}}
\newcommand{\Z}{\boldsymbol{Z}}
\newcommand{\B}{\boldsymbol{B}}
\newcommand{\0}{\boldsymbol{0}}
\newcommand{\xx}{\boldsymbol{\xi}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\usepackage{hyperref}
\hypersetup{
colorlinks=true,
citecolor=blue,
}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}[theorem]{Axiom}


\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\black}{\color{black}}

\makeatletter
\@addtoreset{equation}{section}
\makeatother
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2020}{1-48}{4/00}{10/00}{meila00a}{He Li}

% Short headings should be running head and authors last names

\ShortHeadings{Useful Theorems and Results}{He Li}
\firstpageno{1}

\begin{document}

\title{Useful Theorems and Results in Statistics, Optimization, Probability, and More}

\author{\name He Li \\
       \addr Stern School of Business, New York University \\
       hli@stern.nyu.edu
}

\maketitle

\section{Optimization}

\subsection{Convex Optimization}

A function $f$ is called $\alpha$-\emph{strongly convex} if for any $\x, \y$ in its domain,
\begin{align}
\label{eq:convex-1}
\left(\nabla f(\x) - \nabla f(\y) \right)^\top (\x - \y) \geq \alpha \| \x - \y \|_2^2.
\end{align}
An equivalent condition is the following:
\begin{align}
\label{eq:convex-2}
f(\x) - f(\y) \leq \nabla f(\x)^\top (\x - \y) - \frac{\alpha}{2} \| \x - \y \|_2^2.
\end{align}
A continuously differentiable function $f$ is called $\beta$-\emph{smooth} if the gradient of $f$ is $\beta$-Lipschitz continuous, that is
\begin{align}
\label{eq:smooth-1}
\| \nabla f(\x) - \nabla f(\y) \|_* \leq \beta \| \x - \y \|, \; \; \forall \x, \y,
\end{align}
where $\| \cdot \|_*$ is the dual norm of $\| \cdot \|$. Usually we will use $2$-norm for all.

\begin{lemma} If a function $f$ is $\beta$-smooth, then we have
\begin{align}
\label{eq:smooth-2}
&\left(\nabla f(\x) - \nabla f(\y) \right)^\top (\x - \y) \leq \beta \| \x - \y \|_2^2, \\
\label{eq:smooth-3}
&f(\x) - f(\y) \geq \nabla f(\x)^\top (\x - \y) + \frac{\beta}{2} \| \x - \y \|_2^2.
\end{align}
\end{lemma}

\begin{proof} We only prove Inequality~\eqref{eq:smooth-3}. We define $g(t)=f(\x+t(\y-\x))$, by \eqref{eq:smooth-2}
\begin{align*}
g^{\prime}(t)-g^{\prime}(0)=(\nabla f(\x+t(\y-\x))-\nabla f(\x))^\top(\y-\x) \leq t \beta \|\x-\y\|^{2}_2.
\end{align*}
Therefore,
\begin{align*}
f(y)=g(1) &=g(0)+\int_{0}^{1} g^{\prime}(t) \intd t \\
&\leq g(0)+g^{\prime}(0)+\frac{\beta}{2}\|\x-\y\|^{2}_2 \\
&=f(\x)+\nabla f(\x)^\top(\y-\x)+\frac{\beta}{2}\|\x-\y\|^{2}_2.
\end{align*}
\end{proof}
\begin{remark}Notice that the technique $g(t)=f(\x+t(\y-\x))$ can also be used to prove Inequality~\eqref{eq:convex-2}.
\end{remark}

\begin{theorem}[(\emph{co-coercivity}) Lemma 3.11 in \cite{bubeck2014convex}] Let $f$ be $\beta$-smooth and $\alpha$-strongly convex. Then for all $\x,\y$, we have
\begin{align}
\label{eq:co-coercivity}
\left(\nabla f(\x) - \nabla f(\y) \right)^\top (\x - \y) \geq \frac{\alpha \beta}{\alpha+\beta} \| \x - \y \|_2^2 + \frac{1}{\alpha + \beta}  \left \| \nabla f(\x) - \nabla f(\y) \right\|_2^2.
\end{align}
\end{theorem}
\begin{remark} Notice that this theorem can be used in convergence analysis of gradient method,
\begin{align*}
\| \w_{t+1} - \w^*\|_2^2 &= \left \| \w_{t} - \eta \nabla f(\w_t) - \w^* \right\|_2^2 \\
&= \| \w_{t} - \w^* \|^2_2 - 2 \eta \left(\nabla f(\w_t) - \nabla f(\w^*)\right)^\top (\w_t - \w^*) + \eta^2 \left \| \eta \nabla f(\w_t) - \eta \nabla f(\w^*) \right\|_2^2  \\
&\leq \left(1 - \frac{2 \eta \alpha \beta}{\alpha+\beta}\right) \| \x - \y \|_2^2 + \left(\eta^2 - \frac{2 \eta}{\alpha + \beta}\right)  \left \| \nabla f(\x) - \nabla f(\y) \right\|_2^2.
\end{align*}
\end{remark}

\begin{theorem}[Lemma 3.1 in \cite{srebro2010smoothness}] If a non-negative function $f: \mathcal{W} \mapsto \mathbb{R}_{+}$ is $\beta$-smooth w.r.t. some norm $\| \cdot \|$ for some input space $\mathcal{W}$, then we have
\begin{align}
\label{eq:smooth-gradient}
\left \| \nabla f(\w) \right\|_{*}^2 \leq 4 \beta f(\w),
\end{align}
where $\| \cdot \|_*$ is the dual norm of $\| \cdot \|$.
\end{theorem}

\section{Concentration}

\begin{theorem}[\cite{juditsky2008large}] Let $\x_1, \x_2, \ldots, \x_n$ be independent copies of a zero-mean random vector $\x$. Then we have
\begin{align}
\label{eq:iid-concen}
\E \left \| \frac{1}{2} \sum_{i=1}^n \x_i \right\|_2^2 \leq \frac{1}{n} \E \| \x \|_2^2.
\end{align}
\end{theorem}

\begin{remark}[Symmetrization Technique, Equation (4.17) in \cite{wainwright2019high}] When our sequence
\begin{align*}
f(X_1, \ldots, X_n) \triangleq \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^n f(X_i) - \E f(X)\right|
\end{align*}
is a bounded difference sequence with $\frac{c}{n}$, using McDiarmid's inequality, we have
\begin{align*}
 f(X_1, \ldots, X_n) \leq \E f(X_1, \ldots, X_n) + C \sqrt{\frac{\log (1 / \delta)}{n}},
\end{align*}
with probability at least $1 - \delta$. Now consider to bound the $\E f(X_1, \ldots, X_n)$ term using symmetrization argument:
\begin{align*}
\E_X \left[\sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^n f(X_i) - \E f(X)\right|\right] &= \E_X \left[\sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^n \left(f(X_i) - \E_{Y_i} f(Y_i) \right)\right|\right] \\
&= \E_X \left[\sup_{f \in \mathcal{F}} \left| \E_Y \frac{1}{n} \sum_{i=1}^n \left(f(X_i) - f(Y_i) \right)\right|\right] \\
&\leq \E_{X,Y} \left[\sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^n \left(f(X_i) - f(Y_i) \right)\right|\right],
\end{align*}
and let $(\varepsilon_1, \ldots, \varepsilon_n)$ be an i.i.d. sequence of Rademacher variables, independent of $X$ and $Y$. Hence we have,
\begin{align*}
\E_{X,Y} \left[\sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^n \left(f(X_i) - f(Y_i) \right)\right|\right]
&= \E_{X,Y, \varepsilon} \left[\sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^n \varepsilon_i \left(f(X_i) - f(Y_i) \right)\right|\right] \\
&\leq 2 \E_{X, \varepsilon} \left[\sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_{i=1}^n \varepsilon_i f(X_i)\right|\right] := 2 \mathcal{R}_n(\mathcal{F}).
\end{align*}
Here $\mathcal{R}_n(\mathcal{F})$ is the Rademacher complexity of class $\mathcal{F}$ and can be bounded by Dudley's integral entropy \citep[Theorem 5.22]{wainwright2019high} bound and further bounded by its VC dimension \citep[Equation (5.48)]{wainwright2019high}.
\end{remark}

\section{Useful Inequalities}
\begin{align}
\label{eq:ineq-1}
\sqrt{1 - x} \leq 1 - \frac{x}{2}.
\end{align}

\newpage
\bibliography{refs}

\end{document}
