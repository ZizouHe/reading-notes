# Deep Learning

## Architechture

1. Dynamic Routing Between Capsules
2. Matrix Capsules with Em Routing
3. Deep Residual Learning for Image Recognition
4. [Neural Ordinary Differential Equations](../notes/NeuralODE.html)
5. Augmented Neural ODEs

## Deep Causal Inference

1. Causal Effect Inference with Deep Latent-Variable Models
2. Causal Deep Information Bottleneck
3. Learning Representations for Counterfactual Inference
4. Estimating Individual Treatment Effect: Generalization Bounds and Algorithms

## Deep Reinforcement Learning

1. Mastering the game of Go with deep neural networks and tree search
2. Continuous control with deep reinforcement learning
3. Deterministic Policy Gradient Algorithms
4. [Trust Region Policy Optimization](../notes/TRPO.html)

## Generative Adversarial Network

1. Generative Adversarial Nets
2. towards principled methods for training generative adversarial networks
3. Wasserstein GAN
4. Improved Techniques for Training GANs

## Generalization

1. Dark Knowledge
2. Distilling the Knowledge in a Neural Network
3. Understanding Black-box Predictions via Influence Functions
4. Understanding Deep Learning Requires Rethinking Generalization
5. DeepFool: a simple and accurate method to fool deep neural networks
6. [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](../notes/Sharpe_Minima_Exp.html)
7. [Sharp Minima Can Generalize For Deep Nets](../notes/Sharpe_Minima_Works.html)
8. Entropy-sgd: Biasing Gradient Descent into Wide Valleys
9. [Geometry of Optimization and Implicit Regularization in Deep Learning](../notes/pathSGD.html)
10. [Path-sgd: Path-normalized Optimization in Deep Neural Networks](../notes/pathSGD.html)
11. Norm-based capacity control in neural networks
12. [An Empirical Analysis of the Optimization of Deep Network Loss Surfaces](../notes/Empirical_Loss.html)
13. Theory of Deep Learning II: Landscape of the Empirical Risk in Deep Learning
14. Theory of Deep Learning III: Generalization Properties of SGD
15. Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes
16. On Dropout and Nuclear Norm Regularization
17. Deep Neural Networks as Gaussian Processes
18. Neural Tangent Kernel: Convergence and Generalization in Neural Networks

## Information Theory

1. The Information Bottleneck Method
2. Deep Variational Information Bottleneck
3. Deep Learning and the Information Bottleneck Principle
4. Opening the Black Box of Deep Neural Networks via Information
5. On the Information Bottleneck Theory of Deep Learning
6. Estimating Information Flow in Deep Neural Networks

## Measuring Uncertainty

1. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
2. On Calibration of Modern Neural Networks
3. Predicting Good Probabilities With Supervised Learning
4. A Comprehensive Review of Neural Network-based Prediction Intervals and New Advances
5. Lower Upper Bound Estimation Method for Construction of Neural Network-Based Prediction
6. Estimating the Mean and Variance of the Target Probability Distribution
7. Wind Power Interval Prediction Based on Improved PSO and BP Neural Network
8. Prediction intervals for artificial neural networks
9. Practical confidence and prediction intervals

## Meta Learning

1. Learning to Learn by Gradient Descent by Gradient Descent
2. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
3. Optimization as a Model for Few-shot Learning

## Natural Language Processing

1. Teaching Machines to Read and Comprehend
2. Using the output embedding to improve language models
3. Tying word vectors and word classifiers: A loss framework for language modeling
4. Smart Reply: Automated Response Suggestion for Email

## Reinforcement Learning

1. Mastering the Game of Go with Deep Neural Networks and Tree Search
2. Mastering the Game of Go without Human Knowledge

## Semi-supervised Learning

1. [Semi-supervised Learning with Deep Generative Models](https://zizouhe.github.io/stats-and-beyond/contents/semi.html)
2. [Semi-supervised Learning with Ladder Networks](https://zizouhe.github.io/stats-and-beyond/contents/semi.html)
3. [Auxiliary Deep Generative Models](https://zizouhe.github.io/stats-and-beyond/contents/semi.html)
4. [Semi-Supervised Learning with Generative Adversarial Networks](https://zizouhe.github.io/stats-and-beyond/contents/semi.html)
5. [Semi-supervised Learning with GANs: Revisiting Manifold Regularization](https://zizouhe.github.io/stats-and-beyond/contents/semi.html)
6. Data-Efficient Image Recognition with Contrastive Predictive Coding
7. Temporal Ensembling for Semi-Supervised Learning
8. [Good Semi-supervised Learning that Requires a Bad GAN](../notes/Good_Semi_Bad_GAN.html)


## Training Technique

1. Improving neural networks by preventing co-adaptation of feature detectors
2. Dropout: A simple way to prevent neural networks from overfitting
3. An empirical analysis of dropout in piecewise linear networks
4. Understanding Dropout
5. Asynchronous Stochastic Gradient Descent with Delay Compensation
6. Understanding Synthetic Gradients and Decoupled Neural Interfaces
7. Decoupled Neural Interfaces using Synthetic Gradients
8. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
9. How Does Batch Normalization Help Optimization

## Transfer Learning

1. How transferable are features in deep neural networks?
2. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks
3. Neural Style Transfer A Review
4. Image Style Transfer Using Convolutional Neural Networks
5. Perceptual Losses for Real-Time Style Transfer and Super Resolution

## Unsupervised Learning

1. Representation learning with contrastive predictive coding
